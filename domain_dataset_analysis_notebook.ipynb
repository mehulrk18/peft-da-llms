{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T12:35:41.941866Z",
     "start_time": "2024-11-11T12:35:41.038257Z"
    }
   },
   "outputs": [],
   "source": [
    "from dataset_lib import SumDataLoader, SumDomains, preprocessing_data_with_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def generate_datasets(domain, dataset_name, force_download=False, training_samples=2000):\n",
    "    # try:\n",
    "    data = SumDataLoader(domain=domain, dataset_name=dataset_name, training_samples=training_samples, force_download=force_download)\n",
    "\n",
    "    data.loading_dataset_from_hf()\n",
    "    print(data)\n",
    "    data.train_set = data.processing_data_with_training_prompt(data.train_set, preprocess_function=preprocessing_data_with_prompt)\n",
    "    data.validation_set = data.processing_data_with_training_prompt(data.validation_set, preprocess_function=preprocessing_data_with_prompt)\n",
    "\n",
    "    print(\"**Dataset after it is processed with Prompt**\\n\\n{}\".format(data))\n",
    "    return data\n",
    "\n",
    "    # except Exception as e:\n",
    "    #     print(\"!!!!!! CANNOT GENERATE DATASET FOR {} BECAUSE BELOW EXCEPTION CAUGHT !!!!!!\\n\\n\\n{}\".format(name, e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Script to forcefully generate random samples of all dataset's available.\n",
      "Min and Max number of tokens in articles in train dataset of cnndm are 46 and 1848 respectively!\n",
      "DataLoader for Domain: SumDomains.NEWS and Dataset: cnndm\n",
      "**Dataset Stats**\n",
      "Train: Dataset({\n",
      "    features: ['content', 'summary'],\n",
      "    num_rows: 2000\n",
      "})\n",
      "\n",
      "Validation: Dataset({\n",
      "    features: ['content', 'summary'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "\n",
      "Test: Dataset({\n",
      "    features: ['content', 'summary'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "\n",
      "\n",
      "**Dataset after it is processed with Prompt**\n",
      "\n",
      "DataLoader for Domain: SumDomains.NEWS and Dataset: cnndm\n",
      "**Dataset Stats**\n",
      "Train: Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 2000\n",
      "})\n",
      "\n",
      "Validation: Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "\n",
      "Test: Dataset({\n",
      "    features: ['content', 'summary'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_dict = dict(SumDomains.__members__)\n",
    "print(\"Script to forcefully generate random samples of all dataset's available.\")\n",
    "# for value in data_dict.values():\n",
    "cnn = generate_datasets(domain=\"news\", dataset_name=\"cnndm\", force_download=False, training_samples=2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min and Max number of tokens in articles in train dataset of multinews are 20 and 16434 respectively!\n",
      "DataLoader for Domain: SumDomains.NEWS and Dataset: multinews\n",
      "**Dataset Stats**\n",
      "Train: Dataset({\n",
      "    features: ['summary', 'content'],\n",
      "    num_rows: 2000\n",
      "})\n",
      "\n",
      "Validation: Dataset({\n",
      "    features: ['summary', 'content'],\n",
      "    num_rows: 999\n",
      "})\n",
      "\n",
      "Test: Dataset({\n",
      "    features: ['summary', 'content'],\n",
      "    num_rows: 999\n",
      "})\n",
      "\n",
      "\n",
      "**Dataset after it is processed with Prompt**\n",
      "\n",
      "DataLoader for Domain: SumDomains.NEWS and Dataset: multinews\n",
      "**Dataset Stats**\n",
      "Train: Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 2000\n",
      "})\n",
      "\n",
      "Validation: Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 999\n",
      "})\n",
      "\n",
      "Test: Dataset({\n",
      "    features: ['summary', 'content'],\n",
      "    num_rows: 999\n",
      "})\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "multinews = generate_datasets(domain=\"news\", dataset_name=\"multinews\", force_download=False, training_samples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min and Max number of tokens in articles in train dataset of xsum are 4 and 3244 respectively!\n",
      "DataLoader for Domain: SumDomains.NEWS and Dataset: xsum\n",
      "**Dataset Stats**\n",
      "Train: Dataset({\n",
      "    features: ['summary', 'content'],\n",
      "    num_rows: 1999\n",
      "})\n",
      "\n",
      "Validation: Dataset({\n",
      "    features: ['summary', 'content'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "\n",
      "Test: Dataset({\n",
      "    features: ['summary', 'content'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "\n",
      "\n",
      "**Dataset after it is processed with Prompt**\n",
      "\n",
      "DataLoader for Domain: SumDomains.NEWS and Dataset: xsum\n",
      "**Dataset Stats**\n",
      "Train: Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1999\n",
      "})\n",
      "\n",
      "Validation: Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "\n",
      "Test: Dataset({\n",
      "    features: ['summary', 'content'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "x_news = generate_datasets(domain=\"news\", dataset_name=\"xsum\", force_download=False, training_samples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "213dd7f84e924e5caa5ffb97ba2b6d99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d82cb364ce5c440291621d371d31d644",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c2fe501f27a401ba2ff0baf1ed33608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "******* 'newsroom' Dataset for NEWS Domain -  is stored at domains/news/newsroom !!!*******\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76c63d2cc713475ba0ea3472a8562e0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c51fc125a424701867495990ad1acc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fad816b416f6457d9e9856649c23e577",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ceb2e115a3d47ea925bb404bba0743d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dea2cc575c4a4802babc2332ec84d638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a97fe53bc5f433b986d1469528f33ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min and Max number of tokens in articles in train dataset of newsroom are 51 and 7651 respectively!\n",
      "DataLoader for Domain: SumDomains.NEWS and Dataset: newsroom\n",
      "**Dataset Stats**\n",
      "Train: Dataset({\n",
      "    features: ['summary', 'content'],\n",
      "    num_rows: 2000\n",
      "})\n",
      "\n",
      "Validation: Dataset({\n",
      "    features: ['summary', 'content'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "\n",
      "Test: Dataset({\n",
      "    features: ['summary', 'content'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0066e337ce741e2b3285267829689f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ac7a92b85ce45c4a488db0670bcb90b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Dataset after it is processed with Prompt**\n",
      "\n",
      "DataLoader for Domain: SumDomains.NEWS and Dataset: newsroom\n",
      "**Dataset Stats**\n",
      "Train: Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 2000\n",
      "})\n",
      "\n",
      "Validation: Dataset({\n",
      "    features: ['text'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "\n",
      "Test: Dataset({\n",
      "    features: ['summary', 'content'],\n",
      "    num_rows: 1000\n",
      "})\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "news_room = generate_datasets(domain=\"news\", dataset_name=\"newsroom\", force_download=True, training_samples=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "/Users/mehul/TUM/SS24/DomainAdaptationLLMs/data_newsr does not exist. Make sure you insert a manual dir via `datasets.load_dataset('newsroom', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions:   You should download the dataset from https://lil.nlp.cornell.edu/newsroom/download/index.html\n  The webpage requires registration.\n  To unzip the .tar file run `tar -zxvf complete.tar`. To unzip the .gz files\n  run `gunzip train.json.gz` , ...\n  After downloading, please put the files under the following names\n  dev.jsonl, test.jsonl and train.jsonl in a dir of your choice,\n  which will be used as a manual_dir, e.g. `~/.manual_dirs/newsroom`\n  Newsroom can then be loaded via:\n  `datasets.load_dataset(\"newsroom\", data_dir=\"~/.manual_dirs/newsroom\")`.\n  ",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mFileNotFoundError\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[10], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mdatasets\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m load_dataset, Dataset, DatasetDict, load_from_disk\n\u001B[0;32m----> 3\u001B[0m ld \u001B[38;5;241m=\u001B[39m \u001B[43mload_dataset\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mlil-lab/newsroom\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdata_dir\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mdata_newsr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrust_remote_code\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;66;03m# import tarfile\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;66;03m# with tarfile.open(\"domains/news/newsroom-release.tar\", \"r\") as tar:\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;66;03m#     tar.extractall(\"domains/news/newsroom_extracted\")\u001B[39;00m\n",
      "File \u001B[0;32m~/TUM/SS24/DomainAdaptationLLMs/311_venv/lib/python3.11/site-packages/datasets/load.py:2154\u001B[0m, in \u001B[0;36mload_dataset\u001B[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001B[0m\n\u001B[1;32m   2151\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m builder_instance\u001B[38;5;241m.\u001B[39mas_streaming_dataset(split\u001B[38;5;241m=\u001B[39msplit)\n\u001B[1;32m   2153\u001B[0m \u001B[38;5;66;03m# Download and prepare data\u001B[39;00m\n\u001B[0;32m-> 2154\u001B[0m \u001B[43mbuilder_instance\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdownload_and_prepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2155\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2156\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdownload_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdownload_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2157\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverification_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverification_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2158\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnum_proc\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mnum_proc\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2159\u001B[0m \u001B[43m    \u001B[49m\u001B[43mstorage_options\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstorage_options\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2160\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2162\u001B[0m \u001B[38;5;66;03m# Build dataset for splits\u001B[39;00m\n\u001B[1;32m   2163\u001B[0m keep_in_memory \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m   2164\u001B[0m     keep_in_memory \u001B[38;5;28;01mif\u001B[39;00m keep_in_memory \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m is_small_dataset(builder_instance\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size)\n\u001B[1;32m   2165\u001B[0m )\n",
      "File \u001B[0;32m~/TUM/SS24/DomainAdaptationLLMs/311_venv/lib/python3.11/site-packages/datasets/builder.py:924\u001B[0m, in \u001B[0;36mDatasetBuilder.download_and_prepare\u001B[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001B[0m\n\u001B[1;32m    922\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m num_proc \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    923\u001B[0m     prepare_split_kwargs[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mnum_proc\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m num_proc\n\u001B[0;32m--> 924\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_download_and_prepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    925\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdl_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdl_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    926\u001B[0m \u001B[43m    \u001B[49m\u001B[43mverification_mode\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverification_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    927\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mprepare_split_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    928\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mdownload_and_prepare_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    929\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    930\u001B[0m \u001B[38;5;66;03m# Sync info\u001B[39;00m\n\u001B[1;32m    931\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39mdataset_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28msum\u001B[39m(split\u001B[38;5;241m.\u001B[39mnum_bytes \u001B[38;5;28;01mfor\u001B[39;00m split \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minfo\u001B[38;5;241m.\u001B[39msplits\u001B[38;5;241m.\u001B[39mvalues())\n",
      "File \u001B[0;32m~/TUM/SS24/DomainAdaptationLLMs/311_venv/lib/python3.11/site-packages/datasets/builder.py:1648\u001B[0m, in \u001B[0;36mGeneratorBasedBuilder._download_and_prepare\u001B[0;34m(self, dl_manager, verification_mode, **prepare_splits_kwargs)\u001B[0m\n\u001B[1;32m   1647\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_download_and_prepare\u001B[39m(\u001B[38;5;28mself\u001B[39m, dl_manager, verification_mode, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mprepare_splits_kwargs):\n\u001B[0;32m-> 1648\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_download_and_prepare\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1649\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdl_manager\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1650\u001B[0m \u001B[43m        \u001B[49m\u001B[43mverification_mode\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1651\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcheck_duplicate_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mverification_mode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mVerificationMode\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mBASIC_CHECKS\u001B[49m\n\u001B[1;32m   1652\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mverification_mode\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[43mVerificationMode\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mALL_CHECKS\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1653\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mprepare_splits_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   1654\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/TUM/SS24/DomainAdaptationLLMs/311_venv/lib/python3.11/site-packages/datasets/builder.py:978\u001B[0m, in \u001B[0;36mDatasetBuilder._download_and_prepare\u001B[0;34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001B[0m\n\u001B[1;32m    976\u001B[0m split_dict \u001B[38;5;241m=\u001B[39m SplitDict(dataset_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset_name)\n\u001B[1;32m    977\u001B[0m split_generators_kwargs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_make_split_generators_kwargs(prepare_split_kwargs)\n\u001B[0;32m--> 978\u001B[0m split_generators \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_split_generators\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdl_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43msplit_generators_kwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    980\u001B[0m \u001B[38;5;66;03m# Checksums verification\u001B[39;00m\n\u001B[1;32m    981\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m verification_mode \u001B[38;5;241m==\u001B[39m VerificationMode\u001B[38;5;241m.\u001B[39mALL_CHECKS \u001B[38;5;129;01mand\u001B[39;00m dl_manager\u001B[38;5;241m.\u001B[39mrecord_checksums:\n",
      "File \u001B[0;32m~/.cache/huggingface/modules/datasets_modules/datasets/lil-lab--newsroom/0de29640d173dddeab4d626e8ee328c9170949d0613c72900876d6f5dd789a41/newsroom.py:113\u001B[0m, in \u001B[0;36mNewsroom._split_generators\u001B[0;34m(self, dl_manager)\u001B[0m\n\u001B[1;32m    111\u001B[0m data_dir \u001B[38;5;241m=\u001B[39m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mabspath(os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexpanduser(dl_manager\u001B[38;5;241m.\u001B[39mmanual_dir))\n\u001B[1;32m    112\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m os\u001B[38;5;241m.\u001B[39mpath\u001B[38;5;241m.\u001B[39mexists(data_dir):\n\u001B[0;32m--> 113\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mFileNotFoundError\u001B[39;00m(\n\u001B[1;32m    114\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdata_dir\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not exist. Make sure you insert a manual dir via `datasets.load_dataset(\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnewsroom\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmanual_download_instructions\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    115\u001B[0m     )\n\u001B[1;32m    116\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [\n\u001B[1;32m    117\u001B[0m     datasets\u001B[38;5;241m.\u001B[39mSplitGenerator(\n\u001B[1;32m    118\u001B[0m         name\u001B[38;5;241m=\u001B[39mdatasets\u001B[38;5;241m.\u001B[39mSplit\u001B[38;5;241m.\u001B[39mTRAIN,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    128\u001B[0m     ),\n\u001B[1;32m    129\u001B[0m ]\n",
      "\u001B[0;31mFileNotFoundError\u001B[0m: /Users/mehul/TUM/SS24/DomainAdaptationLLMs/data_newsr does not exist. Make sure you insert a manual dir via `datasets.load_dataset('newsroom', data_dir=...)` that includes files unzipped from the reclor zip. Manual download instructions:   You should download the dataset from https://lil.nlp.cornell.edu/newsroom/download/index.html\n  The webpage requires registration.\n  To unzip the .tar file run `tar -zxvf complete.tar`. To unzip the .gz files\n  run `gunzip train.json.gz` , ...\n  After downloading, please put the files under the following names\n  dev.jsonl, test.jsonl and train.jsonl in a dir of your choice,\n  which will be used as a manual_dir, e.g. `~/.manual_dirs/newsroom`\n  Newsroom can then be loaded via:\n  `datasets.load_dataset(\"newsroom\", data_dir=\"~/.manual_dirs/newsroom\")`.\n  "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SumDomains.NEWS\n"
     ]
    }
   ],
   "source": [
    "from dataset_lib import *\n",
    "print(SumDomains(\"news\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T14:21:51.710157Z",
     "start_time": "2024-11-11T14:21:50.209750Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/mehul/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from analysing_data import *\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "samples = 100\n",
    "# metrics = get_domain_similarity_metrics(src_domain=SumDomains.NEWS.name.lower(), source=\"cnndm\", tgt_domain=SumDomains.NEWS.name.lower(), target=\"cnndm\", da=\"in-domain-adapt\", num_samples = 100)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T14:21:54.798343Z",
     "start_time": "2024-11-11T14:21:52.856767Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: cnndm\n"
     ]
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5c90e2c438bb4d67a62516709529806a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f8e496d736c543faa3622ae6865e78d7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: cnndm\n",
      "Processing cnndm data.\n",
      "Processing cnndm data.\n",
      "CNN DM - in-domain ->  {'word-overlap': 0.11100532508871278, 'vocab-overlap': 0.6246240224583918, 'relevance-overlap': np.float64(0.12663228464098988), 'kl-divergence': 65.04671095121442}\n"
     ]
    }
   ],
   "source": [
    "cnn_metrics = get_domain_similarity_metrics(src_domain=SumDomains.NEWS.name.lower(), source=\"cnndm\", tgt_domain=SumDomains.NEWS.name.lower(), target=\"cnndm\", da=\"in-domain-adapt\", num_samples = samples)\n",
    "print(\"CNN DM - in-domain -> \", cnn_metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T14:30:55.945668Z",
     "start_time": "2024-11-11T14:21:59.184832Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: multinews\n"
     ]
    },
    {
     "data": {
      "text/plain": "Filter:   0%|          | 0/500 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7453f016522745078b103a2f6ac944cc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Map:   0%|          | 0/500 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6c73842777264793918d3e85fa521101"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: multinews\n",
      "Processing multinews data.\n",
      "Processing multinews data.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[4], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m multinews_metrics \u001B[38;5;241m=\u001B[39m \u001B[43mget_domain_similarity_metrics\u001B[49m\u001B[43m(\u001B[49m\u001B[43msrc_domain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mSumDomains\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mNEWS\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlower\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msource\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmultinews\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtgt_domain\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mSumDomains\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mNEWS\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mname\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlower\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtarget\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mmultinews\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mda\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43min-domain-adapt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnum_samples\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43msamples\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMultiNews - in-domain -> \u001B[39m\u001B[38;5;124m\"\u001B[39m, multinews_metrics)\n",
      "File \u001B[0;32m~/TUM/SS24/DomainAdaptationLLMs/dataset_analysis/data_anaylsis.py:133\u001B[0m, in \u001B[0;36mget_domain_similarity_metrics\u001B[0;34m(src_domain, source, tgt_domain, target, da, num_samples)\u001B[0m\n\u001B[1;32m    131\u001B[0m S \u001B[38;5;241m=\u001B[39m Domain(file_paths\u001B[38;5;241m=\u001B[39ms_dval, file_names\u001B[38;5;241m=\u001B[39msource, client\u001B[38;5;241m=\u001B[39mclient)\n\u001B[1;32m    132\u001B[0m T \u001B[38;5;241m=\u001B[39m Domain(file_paths\u001B[38;5;241m=\u001B[39mt_dval, file_names\u001B[38;5;241m=\u001B[39mtarget, client\u001B[38;5;241m=\u001B[39mclient) \u001B[38;5;66;03m#, unique=True)\u001B[39;00m\n\u001B[0;32m--> 133\u001B[0m ST \u001B[38;5;241m=\u001B[39m \u001B[43mSimilarity\u001B[49m\u001B[43m(\u001B[49m\u001B[43mS\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mT\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    135\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mword-overlap\u001B[39m\u001B[38;5;124m\"\u001B[39m: ST\u001B[38;5;241m.\u001B[39mword_overlap,\n\u001B[1;32m    136\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mvocab-overlap\u001B[39m\u001B[38;5;124m\"\u001B[39m: ST\u001B[38;5;241m.\u001B[39mvocab_overlap,\n\u001B[1;32m    137\u001B[0m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrelevance-overlap\u001B[39m\u001B[38;5;124m\"\u001B[39m: ST\u001B[38;5;241m.\u001B[39mrelevance_overlap,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    140\u001B[0m \u001B[38;5;66;03m# \"js-divergence\": ST.js_divergence,\u001B[39;00m\n\u001B[1;32m    141\u001B[0m }\n",
      "File \u001B[0;32m~/TUM/SS24/DomainAdaptationLLMs/dataset_analysis/similarity.py:53\u001B[0m, in \u001B[0;36mSimilarity.__init__\u001B[0;34m(self, source, target, client)\u001B[0m\n\u001B[1;32m     49\u001B[0m \u001B[38;5;66;03m# self.js_divergence = compute_js_divergence(\u001B[39;00m\n\u001B[1;32m     50\u001B[0m \u001B[38;5;66;03m#     self.target.token_frequencies, self.source.token_frequencies\u001B[39;00m\n\u001B[1;32m     51\u001B[0m \u001B[38;5;66;03m# )\u001B[39;00m\n\u001B[1;32m     52\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mvocab_overlap \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_vocab_overlap()\n\u001B[0;32m---> 53\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mword_overlap \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_word_overlap\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mrelevance_overlap \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcompute_relevance_overlap()\n",
      "File \u001B[0;32m~/TUM/SS24/DomainAdaptationLLMs/dataset_analysis/similarity.py:98\u001B[0m, in \u001B[0;36mSimilarity.compute_word_overlap\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     96\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m word \u001B[38;5;129;01min\u001B[39;00m Tj_unique:\n\u001B[1;32m     97\u001B[0m             Si_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(Si) \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(Si)\n\u001B[0;32m---> 98\u001B[0m             gamma \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m Si\u001B[38;5;241m.\u001B[39mcount(word) \u001B[38;5;241m/\u001B[39m Si_size\n\u001B[1;32m     99\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m gamma \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39msource\u001B[38;5;241m.\u001B[39mdataset_size \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtarget\u001B[38;5;241m.\u001B[39mdataset_size\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "multinews_metrics = get_domain_similarity_metrics(src_domain=SumDomains.NEWS.name.lower(), source=\"multinews\", tgt_domain=SumDomains.NEWS.name.lower(), target=\"multinews\", da=\"in-domain-adapt\", num_samples = samples)\n",
    "print(\"MultiNews - in-domain -> \", multinews_metrics)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T14:49:22.825157Z",
     "start_time": "2024-11-11T14:30:55.943743Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "xsum_metrics = get_domain_similarity_metrics(src_domain=SumDomains.NEWS.name.lower(), source=\"xsum\", tgt_domain=SumDomains.NEWS.name.lower(), target=\"xsum\", da=\"in-domain-adapt\", num_samples = samples)\n",
    "print(\"X Sum - in-domain -> \", xsum_metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "newsroom_metrics = get_domain_similarity_metrics(src_domain=SumDomains.NEWS.name.lower(), source=\"newsroom\", tgt_domain=SumDomains.NEWS.name.lower(), target=\"newsroom\", da=\"in-domain-adapt\", num_samples = samples)\n",
    "print(\"NewsRoom - in-domain -> \", newsroom_metrics)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing cnndm with cnndm\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: cnndm\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: cnndm\n",
      "Processing cnndm data.\n",
      "Processing cnndm data.\n",
      "\n",
      "** cnndm - in-domain-adapt - cnndm ==>  {'word-overlap': 0.1099343998989453, 'vocab-overlap': 0.5846867749419954, 'relevance-overlap': np.float64(0.12704865527095985), 'kl-divergence': 139.09649312395067}\n",
      "Comparing cnndm with multinews\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: cnndm\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: multinews\n",
      "Processing cnndm data.\n",
      "Processing multinews data.\n",
      "\n",
      "** cnndm - withing-domain-adapt - multinews ==>  {'word-overlap': 0.17716335487321833, 'vocab-overlap': 0.5752530199151159, 'relevance-overlap': np.float64(0.20713901200780532), 'kl-divergence': 159.81343400414158}\n",
      "Comparing cnndm with xsum\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: cnndm\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: xsum\n",
      "Processing cnndm data.\n",
      "Processing xsum data.\n",
      "\n",
      "** cnndm - withing-domain-adapt - xsum ==>  {'word-overlap': 0.0724299460633603, 'vocab-overlap': 0.5120567375886524, 'relevance-overlap': np.float64(0.0857432883699897), 'kl-divergence': 182.5817766532247}\n",
      "Comparing cnndm with newsroom\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: cnndm\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: newsroom\n",
      "Processing cnndm data.\n",
      "Processing newsroom data.\n",
      "\n",
      "** cnndm - withing-domain-adapt - newsroom ==>  {'word-overlap': 0.10477622494782064, 'vocab-overlap': 0.5412508316699933, 'relevance-overlap': np.float64(0.12285485633155574), 'kl-divergence': 175.60095727026496}\n",
      "Comparing multinews with cnndm\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: multinews\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: cnndm\n",
      "Processing multinews data.\n",
      "Processing cnndm data.\n",
      "\n",
      "** multinews - withing-domain-adapt - cnndm ==>  {'word-overlap': 0.103794924043434, 'vocab-overlap': 0.5840238647663242, 'relevance-overlap': np.float64(0.11783513977156472), 'kl-divergence': 101.15724446819137}\n",
      "Comparing multinews with multinews\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: multinews\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: multinews\n",
      "Processing multinews data.\n",
      "Processing multinews data.\n",
      "\n",
      "** multinews - in-domain-adapt - multinews ==>  {'word-overlap': 0.17267339016747144, 'vocab-overlap': 0.6030035912504081, 'relevance-overlap': np.float64(0.19621475018249698), 'kl-divergence': 103.90976158672588}\n",
      "Comparing multinews with xsum\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: multinews\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: xsum\n",
      "Processing multinews data.\n",
      "Processing xsum data.\n",
      "\n",
      "** multinews - withing-domain-adapt - xsum ==>  {'word-overlap': 0.06732064660387334, 'vocab-overlap': 0.5187943262411348, 'relevance-overlap': np.float64(0.07625824406029623), 'kl-divergence': 131.70430056790505}\n",
      "Comparing multinews with newsroom\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: multinews\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: newsroom\n",
      "Processing multinews data.\n",
      "Processing newsroom data.\n",
      "\n",
      "** multinews - withing-domain-adapt - newsroom ==>  {'word-overlap': 0.09875691481147611, 'vocab-overlap': 0.5668662674650699, 'relevance-overlap': np.float64(0.11154568943433561), 'kl-divergence': 123.06551891137731}\n",
      "Comparing xsum with cnndm\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: xsum\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: cnndm\n",
      "Processing xsum data.\n",
      "Processing cnndm data.\n",
      "\n",
      "** xsum - withing-domain-adapt - cnndm ==>  {'word-overlap': 0.10384198232691802, 'vocab-overlap': 0.47862114683460394, 'relevance-overlap': np.float64(0.12461939281236377), 'kl-divergence': 200.88184298846716}\n",
      "Comparing xsum with multinews\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: xsum\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: multinews\n",
      "Processing xsum data.\n",
      "Processing multinews data.\n",
      "\n",
      "** xsum - withing-domain-adapt - multinews ==>  {'word-overlap': 0.16533770405076365, 'vocab-overlap': 0.4776363042768528, 'relevance-overlap': np.float64(0.1974154505856824), 'kl-divergence': 214.3551729427197}\n",
      "Comparing xsum with xsum\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: xsum\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: xsum\n",
      "Processing xsum data.\n",
      "Processing xsum data.\n",
      "\n",
      "** xsum - in-domain-adapt - xsum ==>  {'word-overlap': 0.07772056612069747, 'vocab-overlap': 0.48829787234042554, 'relevance-overlap': np.float64(0.094278463268413), 'kl-divergence': 186.35098560194916}\n",
      "Comparing xsum with newsroom\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: xsum\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: newsroom\n",
      "Processing xsum data.\n",
      "Processing newsroom data.\n",
      "\n",
      "** xsum - withing-domain-adapt - newsroom ==>  {'word-overlap': 0.10088234163240987, 'vocab-overlap': 0.4823685961410512, 'relevance-overlap': np.float64(0.12069292252325318), 'kl-divergence': 220.29348239434188}\n",
      "Comparing newsroom with cnndm\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: newsroom\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: cnndm\n",
      "Processing newsroom data.\n",
      "Processing cnndm data.\n",
      "\n",
      "** newsroom - withing-domain-adapt - cnndm ==>  {'word-overlap': 0.10051745748176477, 'vocab-overlap': 0.5392774279085184, 'relevance-overlap': np.float64(0.1274818812413802), 'kl-divergence': 138.50984290770916}\n",
      "Comparing newsroom with multinews\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: newsroom\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: multinews\n",
      "Processing newsroom data.\n",
      "Processing multinews data.\n",
      "\n",
      "** newsroom - withing-domain-adapt - multinews ==>  {'word-overlap': 0.1636985731803687, 'vocab-overlap': 0.5563173359451518, 'relevance-overlap': np.float64(0.22888875681170606), 'kl-divergence': 147.86800746519037}\n",
      "Comparing newsroom with xsum\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: newsroom\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: xsum\n",
      "Processing newsroom data.\n",
      "Processing xsum data.\n",
      "\n",
      "** newsroom - withing-domain-adapt - xsum ==>  {'word-overlap': 0.06687629898380391, 'vocab-overlap': 0.5141843971631206, 'relevance-overlap': np.float64(0.07861661797122235), 'kl-divergence': 161.25731399922606}\n",
      "Comparing newsroom with newsroom\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: newsroom\n",
      "DOMAIN_NAME: news\n",
      "DATASET_NAME: newsroom\n",
      "Processing newsroom data.\n",
      "Processing newsroom data.\n",
      "\n",
      "** newsroom - in-domain-adapt - newsroom ==>  {'word-overlap': 0.09867815996319985, 'vocab-overlap': 0.5485695276114437, 'relevance-overlap': np.float64(0.11658485088166498), 'kl-divergence': 160.0406477849944}\n"
     ]
    }
   ],
   "source": [
    "datasets = datasets_info_dict[SumDomains.NEWS].keys()\n",
    "samples = 100\n",
    "\n",
    "for d1 in datasets:\n",
    "    for d2 in datasets:\n",
    "        print(\"Comparing {} with {}\".format(d1, d2))\n",
    "        if d1 == d2:\n",
    "            da = \"in-domain-adapt\"\n",
    "        else:\n",
    "            da = \"withing-domain-adapt\"\n",
    "        metrics = get_domain_similarity_metrics(src_domain=SumDomains.NEWS.name.lower(), source=d1, tgt_domain=SumDomains.NEWS.name.lower(), target=d2, da=da, num_samples = samples)\n",
    "        print(\"\\n** {} - {} - {} ==>  {}\".format(d1, da, d2, metrics))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-11-11T15:01:28.629765Z",
     "start_time": "2024-11-11T14:52:11.537659Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
