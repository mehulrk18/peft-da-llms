# -*- coding: utf-8 -*-
"""just_llama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19K9hAj3vdrmB3tBxBPnhyMV6b8v6JbNA
"""
from peft import prepare_model_for_kbit_training

try:
  from google.colab import drive
  drive.mount('/content/drive')
  dir = "/content/drive/My Drive/Colab Notebooks/"
except Exception as e:
  print("Exceptoion: ", e)
  dir = ""

import os
os.environ["TORCH_USE_CUDA_DSA"] = "1"
os.environ["CUDA_VISIBLE_DEVICES"]="0"
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

print("Environment variable TORCH_USE_CUDA_DSA set. Please restart the runtime.")

# !pip install torch
# !pip install fsspec==2024.2.0
# !pip install datasets==2.18.0
# !pip install transformers==4.39.3
# !pip install bitsandbytes==0.42.0
# !pip install huggingface-hub==0.23.4
# !pip install accelerate==0.30.0
# !pip install adapters
# !pip install peft
# !pip install trl
# !pip install ninja packaging
# # !MAX_JOBS=2 pip install flash-attn --no-build-isolation
# !pip install ninja packaging

import torch
import pandas as pd
from accelerate import Accelerator, init_empty_weights, load_checkpoint_and_dispatch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Seq2SeqTrainingArguments, BitsAndBytesConfig, set_seed
from datasets import load_dataset, load_from_disk, DatasetDict, Dataset
from adapters import AdapterTrainer, LoRAConfig, LlamaAdapterModel, Seq2SeqAdapterTrainer
from transformers import DataCollatorForLanguageModeling, DataCollatorForSeq2Seq



accelerator = Accelerator()
torch.cuda.empty_cache()

"""## Configs"""

dataset_info_dict = {
    "arxiv": {
        "dataset_id": "ccdv/arxiv-summarization",
        "local_path": "domains/arxiv_summarization"
    },
    "pubmed": {
        "dataset_id": "ccdv/pubmed-summarization",
        "local_path": "domains/pubmed_summarization"
    }
}
max_seq_len = 256 # context window # 1024
ddtype = torch.bfloat16 # float32
bits = 4 #8
bf16 = False
bf32 = True
fp16 = False
compute_dtype = torch.bfloat16 # torch.bfloat16 if bf16 else torch.float32

# Model Config
llama31 = "meta-llama/Meta-Llama-3.1-8B-Instruct"
llama3 = "meta-llama/Meta-Llama-3-8B-Instruct"
llama2 = "meta-llama/Llama-2-7b-hf"

device = "cuda" if torch.cuda.is_available() else "cpu"
# device = "mps" if torch.backends.mps.is_available() else "cpu"
print("device: ", device)

if device == "cuda":
  n_gpus = torch.cuda.device_count()
  print("gpus-> ", n_gpus)

from huggingface_hub import login
login(token="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")

"""# LLama Model"""

model_id = llama3 # llama2

# Load 4-bit quantized model
model = LlamaAdapterModel.from_pretrained( #AutoModelForCausalLM.from_pretrained(
    model_id,
    device_map="auto",
    quantization_config=BitsAndBytesConfig(
        load_in_4bit=bits == 4,
        load_in_8bit=bits == 8,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=compute_dtype,
    ),
    torch_dtype=ddtype,
)
model.config.use_cache = False
"""# Tokenizer"""

tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side="right", tokenizer_type="llama",
                                          trust_remote_code=True, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token
model.resize_token_embeddings(len(tokenizer))
model.config.torch_dtype=torch.float32
model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)
# model.config.torch_dtype=torch.float32# (torch.float16 if fp16 else (torch.bfloat16 if bf16 else torch.float16))
# if 'llama' in model_id:
tokenizer.add_special_tokens({
            "eos_token": tokenizer.convert_ids_to_tokens(model.config.eos_token_id),
            "bos_token": tokenizer.convert_ids_to_tokens(model.config.bos_token_id),
            # "unk_token": tokenizer.convert_ids_to_tokens(
            #     model.config.pad_token_id if model.config.pad_token_id != -1 else tokenizer.pad_token_id
            # ),
    })
# else:
#   tokenizer.pad_token = tokenizer.eos_token

"""# Adding Adapter / LoRA"""
import adapters
adapters.init(model)

lora_config = LoRAConfig(
    selfattn_lora=True, intermediate_lora=True, output_lora=True,
    attn_matrices=["q", "k", "v"],
    alpha=16, r=64, dropout=0.1,
)

model.add_adapter("arxiv_adapter", config=lora_config)
# model.add_seq2seq_lm_head("arxiv_adapter")
model.set_active_adapters("arxiv_adapter")
model.train_adapter("arxiv_adapter")

print(model.adapter_summary())

"""### To correctly train bottleneck adapters or prefix tuning, uncomment the following lines to move the adapter weights to GPU explicitly:"""

model.adapter_to("arxiv_adapter", device="cuda")

"""Some final preparations for 4bit training: we cast a few parameters to float32 for stability.

# Loading Dataset - Arxiv
"""

for param in model.parameters():
    if param.ndim == 1:
        # cast the small parameters (e.g. layernorm) to fp32 for stability
        param.data = param.data.to(torch.float32)

# Enable gradient checkpointing to reduce required memory if needed
model.gradient_checkpointing_enable()
model.enable_input_require_grads()

# class CastOutputToFloat(torch.nn.Sequential):
#     def forward(self, x): return super().forward(x).to(torch.float32)
# model.causal_lm = CastOutputToFloat(model.causal_lm)

# Dataset helper
def loading_dataset(dataset_name, force_run=False):
  local_path = dir + dataset_info_dict[dataset_name]["local_path"]
  if not os.path.exists(local_path) or force_run:
    dataset_id = dataset_info_dict[dataset_name]["dataset_id"]
    dataset = load_dataset(path=dataset_id, streaming=True, trust_remote_code=True)
    dataset_dict = {}
    for split, data in dataset.items():
        # if split == "train":
        data = list(data)[:500] # TODO: selecting random 1k or 5k or 10k
        df = pd.DataFrame(data)
        dataset_dict[split] = Dataset.from_pandas(df) # dataset[split].to_pandas())
    _dataset = DatasetDict(dataset_dict)
    _dataset.save_to_disk(local_path)
    return _dataset
  else:
    dataset = load_from_disk(local_path)
    return dataset

def preprocess_dataset(_data):
  return {"text": _data["article"], "summary": _data["abstract"]}

def tokenization_process(input_data):
    input_data.pop("article", None)
    input_data.pop("abstract", None)
    inputs = tokenizer(input_data['text'], max_length=max_seq_len, truncation=True, padding="max_length",
                       return_tensors="pt")
    targets = tokenizer(input_data['summary'], max_length=max_seq_len, truncation=True, padding="max_length",
                        return_tensors="pt")

    # For causal LM, concatenate the input and output for training
    input_ids = inputs['input_ids']
    labels = targets['input_ids']
    return {'input_ids': input_ids, 'attention_mask': inputs['attention_mask'], 'labels': labels}


dataset_arxiv = loading_dataset("arxiv") #, force_run=True)
# dataset_pubmed = loading_dataset("pubmed")


# Encode the input data
dataset_arxiv = dataset_arxiv.map(preprocess_dataset, batched=True)#, num_proc=os.cpu_count())
dataset_arxiv = dataset_arxiv.map(tokenization_process, batched=True)# , num_proc=os.cpu_count())
print("Arxiv: ", dataset_arxiv)
# dataset_pubmed = dataset_pubmed.map(preprocess_dataset, batched=True)
# print("Pubmed: ", dataset_pubmed)

"""# Training"""
set_seed(42)
tr_arxiv = dataset_arxiv["train"]    #  [:500]
val_arxiv = dataset_arxiv["validation"]  # [:500]

training_args = Seq2SeqTrainingArguments( #Seq2SeqTrainingArguments
output_dir=dir+'results',
    evaluation_strategy="epoch",
    learning_rate=2e-4,
    # gradient_checkpointing=True,
    optim="paged_adamw_32bit",
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=16,
    num_train_epochs=1,
    weight_decay=0.01,
    bf16=bf16,
    fp16=True,
    tf32=False,
    logging_dir=dir+"logs",
    logging_steps=1,
    lr_scheduler_type="constant",
    disable_tqdm=False

    # output_dir="results/llama_qlora",
    # per_device_train_batch_size=1,
    # per_device_eval_batch_size=1,
    # evaluation_strategy="steps",
    # logging_steps=10,
    # save_steps=500,
    # eval_steps=187,
    # num_train_epochs=1,
    # save_total_limit=3,
    # gradient_accumulation_steps=16,
    # max_steps=30,
    # lr_scheduler_type="constant",
    # optim="paged_adamw_32bit",
    # learning_rate=0.0002,
    # group_by_length=True,
    # bf16=True,
    # warmup_ratio=0.03,
    # max_grad_norm=0.3
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding="max_length", max_length=max_seq_len, label_pad_token_id=-100)
# data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
trainer = Seq2SeqAdapterTrainer( # AdapterTrainer
    model=model,
    tokenizer=tokenizer,
    data_collator=data_collator,
    train_dataset=tr_arxiv,
    eval_dataset=val_arxiv,
    args=training_args,
)

from transformers import TrainerCallback
trainer.add_callback(TrainerCallback)
# trainer = accelerator.prepare(trainer)
trainer.train()

results = trainer.evaluate()
print(results)

"""# Inference"""

# from transformers import logging
# logging.set_verbosity(logging.CRITICAL)

# def summarization_model(model, text: str):
#     batch = tokenizer(f"{text}", return_tensors="pt")
#     batch = batch.to(model.device)

#     model.eval()
#     with torch.inference_mode(), torch.cuda.amp.autocast():
#         output_tokens = model.generate(**batch, max_new_tokens=50)

#     return tokenizer.decode(output_tokens[0], skip_special_tokens=True)

# print(summarization_model(model, "Text"))

"""# Merge LoRA weights"""

model.merge_adapter("arxiv_adapter")

# print(summarization_model(model, "Text"))

# from transformers import TrainerCallback
# trainer.add_callback(TrainerCallback)

# trainer = accelerator.prepare(trainer)

# new_articles = ["New article text here..."]

gen_art = dataset_arxiv["test"]

i = 1
texts ,sums = [], []
def summarization_model(model, text, true_summary=""):
  # for k, t in enumerate(gen_art):
  #   if k < i:
  #     texts.append(t["text"])
  #     sums.append(t["summary"])




  inputs = tokenizer(text, return_tensors="pt", max_length=max_seq_len, truncation=True, padding="max_length")
  inputs = inputs.to(model.device)
  # model.eval()
  outputs = model.generate(inputs.input_ids, max_length=1024, num_beams=5, early_stopping=True)
  pred_summary = tokenizer.decode(outputs, skip_special_tokens=True)
  # for s1, s2 in zip(sums, summaries):
  if len(true_summary) > 0:
    print("actual: ", true_summary)
  print("\n\npred: ", pred_summary)

# gen_art[4].keys()
# print(gen_art[4]["text"])
# print("************(************(************(************(************")
# print(gen_art[4]["summary"])

summarization_model(model, gen_art[4]["text"],gen_art[4]["summary"])