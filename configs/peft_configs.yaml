lora:
  selfattn_lora: True
  intermediate_lora: True
  output_lora: True
  alpha: 16
  r: 32
  dropout: 0.1
  attention_matrices:
    - "q"
    - "k"
    - "v"

ia3:
  selfattn_lora: True
  intermediate_lora: True
  output_lora: True
  attn_matrices:
    - "q_proj"
    - "k_proj"
    - "v_proj"
  alpha: 1
  r: 1

double_seq_bottleneck:
  mh_adapter: True
  output_adapter: True
  reduction_factor: 16
  non_linearity: "relu"

seq_bottleneck:
  mh_adapter: True
  output_adapter: True
  reduction_factor: 16
  non_linearity: "relu"

parallel_bottleneck:
  mh_adapter: True
  output_adapter: True
  reduction_factor: 16
  non_linearity: "relu"
  scaling: 2


