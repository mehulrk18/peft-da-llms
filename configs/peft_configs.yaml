ah:
  lora:
    selfattn_lora: True
    intermediate_lora: True
#    output_lora: True
    alpha: 8
    r: 32
    dropout: 0.1
    attn_matrices:
      - "q_proj"
      - "v_proj"

  ia3:
    selfattn_lora: True
#    intermediate_lora: True
#    output_lora: True
    attn_matrices:
      - "q_proj"
      - "k_proj"
      - "v_proj"
    alpha: 1
    r: 1

  double_seq_bottleneck:
    mh_adapter: True
    output_adapter: True
    reduction_factor: 16
    non_linearity: "relu"

  seq_bottleneck:
    mh_adapter: True
    output_adapter: True
    reduction_factor: 16
    non_linearity: "relu"

  parallel_bottleneck:
    mh_adapter: True
    output_adapter: True
    reduction_factor: 16
    non_linearity: "relu"
    scaling: 2


hf:
  lora:
    inference_mode: False
    r: 32
    target_modules:
      - "q_proj"
      - "v_proj"
    lora_alpha: 8
    bias: "lora_only"
    lora_dropout: 0.1
#      - "down_proj"
#    feedforward_modules:
#      - "down_proj"

  ia3:
    peft_type: "IA3"
    init_ia3_weights: True
    inference_mode: False
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
      - "down_proj"
    feedforward_modules:
      - "down_proj"
