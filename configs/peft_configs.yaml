ah:
  lora:
    selfattn_lora: True
    intermediate_lora: True
    output_lora: True
    alpha: 16
    r: 64
    dropout: 0.1
    attn_matrices:
      - "q_proj"
      - "v_proj"

  ia3:
    selfattn_lora: True
    intermediate_lora: True
    output_lora: True
    attn_matrices:
      - "q_proj"
      - "v_proj"
    alpha: 1
    r: 1

  double_seq_bottleneck:
    mh_adapter: True
    output_adapter: True
    reduction_factor: 16
    non_linearity: "relu"

  seq_bottleneck:
    mh_adapter: True
    output_adapter: True
    reduction_factor: 16
    non_linearity: "relu"

  parallel_bottleneck:
    mh_adapter: True
    output_adapter: True
    reduction_factor: 16
    non_linearity: "relu"
    scaling: 2


hf:
  lora:
    inference_mode: False
    r: 64
    target_modules:
      - "q_proj"
      - "k_proj"
      - "v_proj"
    lora_alpha: 16
    bias: "none"
    lora_dropout: 0.1
