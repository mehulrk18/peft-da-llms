# -*- coding: utf-8 -*-
"""just_llama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19K9hAj3vdrmB3tBxBPnhyMV6b8v6JbNA
"""
import bitsandbytes
from peft import prepare_model_for_kbit_training
from torch.utils.data import DataLoader
from tqdm import tqdm

try:
  from google.colab import drive
  drive.mount('/content/drive')
  dir = "/content/drive/My Drive/Colab Notebooks/"
except Exception as e:
  print("Exceptoion: ", e)
  dir = ""

import os
os.environ["TORCH_USE_CUDA_DSA"] = "1"
os.environ["CUDA_VISIBLE_DEVICES"]="0"
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

# print("Environment variable TORCH_USE_CUDA_DSA set. Please restart the runtime.")

# !pip install torch
# !pip install fsspec==2024.2.0
# !pip install datasets==2.18.0
# !pip install transformers==4.39.3
# !pip install bitsandbytes==0.42.0
# !pip install huggingface-hub==0.23.4
# !pip install accelerate==0.30.0
# !pip install adapters
# !pip install peft
# !pip install trl
# !pip install ninja packaging
# # !MAX_JOBS=2 pip install flash-attn --no-build-isolation
# !pip install ninja packaging

import torch
import pandas as pd
from accelerate import Accelerator, init_empty_weights, load_checkpoint_and_dispatch
from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, Seq2SeqTrainingArguments, \
    BitsAndBytesConfig, set_seed, Seq2SeqTrainer
from datasets import load_dataset, load_from_disk, DatasetDict, Dataset
from adapters import AdapterTrainer, LoRAConfig, LlamaAdapterModel, Seq2SeqAdapterTrainer
from transformers import DataCollatorForLanguageModeling, DataCollatorForSeq2Seq



accelerator = Accelerator()
torch.cuda.empty_cache()

"""## Configs"""

dataset_info_dict = {
    "arxiv": {
        "dataset_id": "ccdv/arxiv-summarization",
        "local_path": "domains/arxiv_summarization"
    },
    "pubmed": {
        "dataset_id": "ccdv/pubmed-summarization",
        "local_path": "domains/pubmed_summarization"
    }
}
max_seq_len = 256 # context window # 1024
ddtype = torch.float32 # bfloat16
bits = 4 #8
bf16 = False
bf32 = False
fp16 = True
compute_dtype = torch.float32 # torch.bfloat16 if bf16 else torch.float32

# Model Config
llama31 = "meta-llama/Meta-Llama-3.1-8B-Instruct" # works only with transformers==4.43.3
llama3 = "meta-llama/Meta-Llama-3-8B-Instruct"
llama2 = "meta-llama/Llama-2-7b-hf"

device = "cuda" if torch.cuda.is_available() else "cpu"
# device = "mps" if torch.backends.mps.is_available() else "cpu"
print("device: ", device)

if device == "cuda":
  n_gpus = torch.cuda.device_count()
  print("gpus-> ", n_gpus)

from huggingface_hub import login
login(token="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")

"""# LLama Model"""

model_id = llama3 # llama2

# Load 4-bit quantized model
model = AutoModelForCausalLM.from_pretrained( #LlamaAdapterModel.from_pretrained(
    model_id,
    device_map="auto",
    quantization_config=BitsAndBytesConfig(
        load_in_4bit=bits == 4,
        load_in_8bit=bits == 8,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_use_double_quant=True,
        bnb_4bit_compute_dtype=compute_dtype,
    ),
    torch_dtype=ddtype,
)
model.config.use_cache = False

setattr(model, 'model_parallel', True)
setattr(model, 'is_parallelizable', True)

"""# Tokenizer"""

tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side="right", tokenizer_type="llama",
                                          trust_remote_code=True, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token
model.resize_token_embeddings(len(tokenizer))
model.config.torch_dtype = torch.float32
# tokenizer.add_special_tokens({
#             "eos_token": tokenizer.convert_ids_to_tokens(model.config.eos_token_id),
#             "bos_token": tokenizer.convert_ids_to_tokens(model.config.bos_token_id),
# })

model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)
model.config.torch_dtype=torch.float32# (torch.float16 if fp16 else (torch.bfloat16 if bf16 else torch.float16))
# if 'llama' in model_id:
# else:
#   tokenizer.pad_token = tokenizer.eos_token

"""# Adding Adapter / LoRA"""
# import adapters
# adapters.init(model)

# lora_config = LoRAConfig(
#     selfattn_lora=True, intermediate_lora=True, output_lora=True,
#     attn_matrices=["q", "k", "v"],
#     alpha=16, r=64, dropout=0.1,
# )

from peft import LoraConfig, TaskType

peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM, inference_mode=False, r=8, #target_modules=["q", "v"],
                         lora_alpha=16, lora_dropout=0.1, modules_to_save=["arxiv_lora"])

# model.add_adapter("arxiv_adapter", config=peft_config)
# model.add_seq2seq_lm_head("arxiv_adapter")
# model.set_active_adapters("arxiv_adapter")
# model.train_adapter("arxiv_adapter")


from peft import get_peft_model

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# print(model.adapter_summary())

"""### To correctly train bottleneck adapters or prefix tuning, uncomment the following lines to move the adapter weights to GPU explicitly:"""

# model.adapter_to("arxiv_adapter", device="cuda")

"""Some final preparations for 4bit training: we cast a few parameters to float32 for stability.

# Loading Dataset - Arxiv
"""

for param in model.parameters():
    if param.ndim == 1:
        # cast the small parameters (e.g. layernorm) to fp32 for stability
        param.data = param.data.to(torch.float32)


# def find_all_linear_names(bits, model):
clsi = bitsandbytes.nn.Linear4bit if bits == 4 else (bitsandbytes.nn.Linear8bitLt if bits == 8 else torch.nn.Linear)
lora_module_names = set()
for name, module in model.named_modules():
    if isinstance(module, clsi):
        names = name.split('.')
        lora_module_names.add(names[0] if len(names) == 1 else names[-1])


if 'lm_head' in lora_module_names: # needed for 16-bit
    lora_module_names.remove('lm_head')
# return list(lora_module_names)
# Enable gradient checkpointing to reduce required memory if needed
model.gradient_checkpointing_enable()
model.enable_input_require_grads()

# class CastOutputToFloat(torch.nn.Sequential):
#     def forward(self, x): return super().forward(x).to(torch.float32)
# model.causal_lm = CastOutputToFloat(model.causal_lm)

# Dataset helper
def loading_dataset(dataset_name, force_run=False):
    local_path = dir + dataset_info_dict[dataset_name]["local_path"]
    if not os.path.exists(local_path) or force_run:
        dataset_id = dataset_info_dict[dataset_name]["dataset_id"]
        dataset = load_dataset(path=dataset_id, streaming=True, trust_remote_code=True)
        dataset_dict = {}
        for split, data in dataset.items():
            # if split == "train":
            data = list(data)[:500] # TODO: selecting random 1k or 5k or 10k
            df = pd.DataFrame(data)
            dataset_dict[split] = Dataset.from_pandas(df) # dataset[split].to_pandas())
        _dataset = DatasetDict(dataset_dict)
        _dataset.save_to_disk(local_path)
        return _dataset
    else:
        dataset = load_from_disk(local_path)
    return dataset

def preprocess_dataset(_data):
    return {"text": _data["article"], "summary": _data["abstract"]}

def tokenization_process(input_data):
    input_data.pop("article", None)
    input_data.pop("abstract", None)
    inputs = tokenizer(input_data['text'], max_length=max_seq_len, truncation=True, padding="max_length",
                       return_tensors="pt")
    targets = tokenizer(input_data['summary'], max_length=max_seq_len, truncation=True, padding="max_length",
                        return_tensors="pt")

    # For causal LM, concatenate the input and output for training
    input_ids = inputs['input_ids']
    labels = targets['input_ids']
    return {'input_ids': input_ids, 'attention_mask': inputs['attention_mask'], 'labels': labels}


dataset_arxiv = loading_dataset("arxiv") #, force_run=True)
# dataset_pubmed = loading_dataset("pubmed")


# Encode the input data
dataset_arxiv = dataset_arxiv.map(preprocess_dataset, batched=True)#, num_proc=os.cpu_count())
dataset_arxiv = dataset_arxiv.map(tokenization_process, batched=True)# , num_proc=os.cpu_count())
# dataset_arxiv.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
print("Arxiv: ", dataset_arxiv)
# dataset_pubmed = dataset_pubmed.map(preprocess_dataset, batched=True)
# print("Pubmed: ", dataset_pubmed)

"""# Training"""
set_seed(42)
tr_arxiv = dataset_arxiv["train"]    #  [:500]
val_arxiv = dataset_arxiv["validation"]  # [:500]
torch.enable_grad()
training_args = Seq2SeqTrainingArguments( #Seq2SeqTrainingArguments
    output_dir="results/llama_qlora",
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    evaluation_strategy="steps",
    logging_steps=10,
    save_steps=500,
    eval_steps=187,
    num_train_epochs=1,
    save_total_limit=3,
    gradient_accumulation_steps=16,
    max_steps=30,
    lr_scheduler_type="constant",
    optim="paged_adamw_32bit",
    learning_rate=0.0002,
    group_by_length=True,
    bf16=True,
    warmup_ratio=0.03,
    max_grad_norm=0.3
)

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding="max_length", max_length=max_seq_len, label_pad_token_id=-100)
# data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
trainer = Seq2SeqTrainer( #Seq2SeqAdapterTrainer( # AdapterTrainer
    model=model,
    tokenizer=tokenizer,
    data_collator=data_collator,
    train_dataset=tr_arxiv,
    eval_dataset=val_arxiv,
    args=training_args,
)

from transformers import TrainerCallback
trainer.add_callback(TrainerCallback)
trainer = accelerator.prepare(trainer)
trainer.train()

results = trainer.evaluate()
print(results)

"""# Save Adapter """

adapter_save_dir = "saved_models/"
model.save_pretrained("{}/arxiv_lora_adapter".format(adapter_save_dir))

"""# Inference"""

# from transformers import logging
# logging.set_verbosity(logging.CRITICAL)

# def summarization_model(model, text: str):
#     batch = tokenizer(f"{text}", return_tensors="pt")
#     batch = batch.to(model.device)

#     model.eval()
#     with torch.inference_mode(), torch.cuda.amp.autocast():
#         output_tokens = model.generate(**batch, max_new_tokens=50)

#     return tokenizer.decode(output_tokens[0], skip_special_tokens=True)

# print(summarization_model(model, "Text"))

"""# Merge LoRA weights"""

# model.merge_adapter("arxiv_adapter")

# print(summarization_model(model, "Text"))

# from transformers import TrainerCallback
# trainer.add_callback(TrainerCallback)

# trainer = accelerator.prepare(trainer)

# new_articles = ["New article text here..."]

gen_art = dataset_arxiv["test"]

i = 1
texts ,sums = [], []


def summarization_model(model, text, true_summary=None):
    inputs = tokenizer(text, return_tensors="pt", max_length=max_seq_len, truncation=True, padding="max_length")
    inputs = inputs.to(model.device)
    # model.eval()
    # outputs = model.generate(inputs.input_ids, max_length=1024, num_beams=5, early_stopping=True)
    outputs = model.generate(
        input_ids=inputs.input_ids,
        attention_mask=inputs.attention_mask,
        max_length=1024,
        num_beams=5,
        early_stopping=True
    )
    # pred_summary = tokenizer.decode(outputs, skip_special_tokens=True)
    pred_summary = tokenizer.decode(outputs[0], skip_special_tokens=True)
    # for s1, s2 in zip(sums, summaries):
    # if len(true_summary) > 0:
    print("actual: ", true_summary)
    print("\n\npred: ", pred_summary)

    return ([true_summary], [pred_summary]) if isinstance(pred_summary, str) else (true_summary, pred_summary)


"""# Rouge Evaluation"""

import evaluate

rouge = evaluate.load("rouge")

print("************(************(************(************(************")
print(gen_art[4].keys()) #gen_art[4]["summary"])

truth, preds = summarization_model(model, gen_art[4]["text"],gen_art[4]["summary"])

print("\n\n\nRouge Scores: ", rouge.compute(references=truth, predictions=preds))