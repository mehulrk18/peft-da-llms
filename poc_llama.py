# -*- coding: utf-8 -*-
"""llama_poc.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/11roqKJB8TUE_otx5hqQ3jKfNRBpW52jQ
"""

# !pip uninstall torch torchvision torchaudio -y

# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --extra-index-url https://download.pytorch.org/whl/nightly/cu118

from google.colab import drive
drive.mount('/content/drive')

import os
os.environ['TORCH_USE_CUDA_DSA'] = '1'

print("Environment variable TORCH_USE_CUDA_DSA set. Please restart the runtime.")

os.getenv("TORCH_USE_CUDA_DSA")

os.environ["CUDA_LAUNCH_BLOCKING"] = '1'
os.getenv("CUDA_LAUNCH_BLOCKING")
# %env TORCH_USE_CUDA_DSA=1

# !pip install torch
!pip install fsspec==2024.2.0
!pip install datasets==2.18.0
!pip install transformers==4.39.3
!pip install bitsandbytes==0.42.0
!pip install huggingface-hub==0.23.4
!pip install accelerate==0.30.0
!pip install peft
!pip install trl
!pip install ninja packaging
# # !MAX_JOBS=2 pip install flash-attn --no-build-isolation
# !pip install ninja packaging

import torch
torch.cuda.empty_cache()

from accelerate import Accelerator, init_empty_weights, load_checkpoint_and_dispatch
accelerator = Accelerator()

# !pip show huggingface_hub
# !pip show accelerate

"""## Configs"""

dataset_info_dict = {
    "arxiv": {
        "dataset_id": "ccdv/arxiv-summarization",
        "local_path": "domains/arxiv_summarization"
    },
    "pubmed": {
        "dataset_id": "ccdv/pubmed-summarization",
        "local_path": "domains/pubmed_summarization"
    }
}

dir = "/content/drive/My Drive/Colab Notebooks/"
max_seq_len = 256 # context window # 1024
ddtype = torch.float16 # float16
bits = 4 #8
bf16 = False
bf32 = True
fp16 = False
compute_dtype = ddtype# torch.float32 # torch.bfloat16 if bf16 else torch.float32

# Model Config
llama3_model_id = "meta-llama/Meta-Llama-3.1-8B-Instruct"  #"meta-llama/Meta-Llama-3-8B-Instruct"
mistral_model_id = "mistralai/Mistral-7B-v0.3" # "mistralai/Mixtral-8x7B-v0.1"  # "meta-llama/Meta-Llama-3-8B-Instruct" #  "meta-llama/Llama-2-7b-hf" # "TheBloke/Llama-2-7B-Chat-GGML"
llama2 = "meta-llama/Llama-2-7b-hf"
dgpt = "distilgpt2"
model_id =  llama2 # llama2

import torch
device = "cuda" if torch.cuda.is_available() else "cpu"
# device = "mps" if torch.backends.mps.is_available() else "cpu"
print("device: ", device)

if device == "cuda":
  n_gpus = torch.cuda.device_count()
  print("gpus-> ", n_gpus)

from huggingface_hub import login
login(token="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")

"""# LLama3 Model"""

from accelerate import disk_offload
from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoModelForSeq2SeqLM, set_seed

add_bnb_config = True
use_flash_attn = True
if add_bnb_config:
  rope_scaling = {
      "type": "linear",
      "factor": 8.0
  }

  model = AutoModelForCausalLM.from_pretrained(
      model_id,
      # cache_dir=args.cache_dir,
      # load_in_4bit=False,
      # load_in_8bit=True,
      device_map="auto",
      # max_memory=max_memory,
      quantization_config=BitsAndBytesConfig(
          load_in_4bit=bits == 4,
          load_in_8bit=bits == 8,
          llm_int8_threshold=6.0,
          llm_int8_has_fp16_weight=True,
          bnb_4bit_compute_dtype=compute_dtype,
          bnb_4bit_use_double_quant=True,
          bnb_4bit_quant_type="nf4",
      ),
      rope_scaling=rope_scaling,
      torch_dtype=ddtype, #(torch.float32 if args.fp16 else (torch.bfloat16 if args.bf16 else torch.float32)),
      trust_remote_code=True,
      use_auth_token=True
  )

  setattr(model, 'model_parallel', True)
  setattr(model, 'is_parallelizable', True)

  model.config.torch_dtype= ddtype # (torch.float32 if args.fp16 else (torch.bfloat16 if args.bf16 else torch.float32))
    # bnb_config = BitsAndBytesConfig(
    #     # load_in_4bit=True,
    #     load
    #     bnb_4bit_use_double_quant=True,
    #     bnb_4bit_quant_type="nf4",
    #     bnb_4bit_compute_dtype=torch.float32,# torch.bfloat16,
    #     llm_int8_threshold=6.0,
    #     llm_int8_has_fp16_weight=True, # False



    #     # llm_int8_threshold=6.0,

    #     # load_in_8bit=True,
    #     # bnb_8bit_use_double_quant=True,
    #     # bnb_8bit_quant_type="nf4",
    #     # bnb_8bit_compute_dtype=torch.bfloat16,
    #     # llm_int8_has_fp16_weight = True


    #     # load_in_8bit_fp32_cpu_offload=True
    #     # bnb_8bit_use_double_quant=True,
    #     # bnb_8bit_quant_type="nf4",
    #     # bnb_8bit_compute_dtype=torch.bfloat16
    # )

    # model = AutoModelForCausalLM.from_pretrained(
    #     model_id,
    #     device_map="auto",
    #     # load_in_4bit=True,
    #     torch_dtype=torch.float32, #torch.bfloat16
    #     # use_flash_attention_2=use_flash_attn,
    #     # attn_implementation="flash_attention_2",
    #     low_cpu_mem_usage=True,
    #     trust_remote_code=True,
    #     # max_memory={0: "15GB"}, # Uncomment this line with you encounter CUDA out of memory errors
    #     quantization_config=bnb_config
    # )
    # disk_offload(model, dir+"saved_models")

else:
  # with init_empty_weights():
  model =  AutoModelForCausalLM.from_pretrained(  # AutoModelForSeq2SeqLM.from_pretrained(
      model_id,
      # device_map="auto",
      torch_dtype=torch.float32,
      low_cpu_mem_usage=True,
      trust_remote_code=True,
      # offload_buffers=True,
      # max_memory={0: "15GB"} # Uncomment this line with you encounter CUDA out of memory errors
  ) # .config

  # with init_empty_weights():
  #   model = AutoModelForCausalLM.from_config(model_config) #, device_map="auto")

  offload_folder = dir+"saved_models"
  # model = load_checkpoint_and_dispatch(model, model_id, device_map="auto")

  # for name, module in model.named_modules():
  #   if isinstance(module, torch.nn.Module):
  #       disk_offload(module, offload_folder)
  # for layer in model.transformer.h:
  #   disk_offload(layer, offload_folder)

  model = model.to(device)
    # model.disk_offload(device)

"""# Tokenizer"""

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side="right", tokenizer_type='llama' if 'llama' in model_id else None, trust_remote_code=True, use_fast=True)
tokenizer.add_special_tokens({'pad_token': '[PAD]'})

model.config.torch_dtype=torch.float32# (torch.float16 if fp16 else (torch.bfloat16 if bf16 else torch.float16))
if 'llama' in model_id:
  tokenizer.add_special_tokens({
              "eos_token": tokenizer.convert_ids_to_tokens(model.config.eos_token_id),
              "bos_token": tokenizer.convert_ids_to_tokens(model.config.bos_token_id),
              # "unk_token": tokenizer.convert_ids_to_tokens(
              #     model.config.pad_token_id if model.config.pad_token_id != -1 else tokenizer.pad_token_id
              # ),
      })
else:
  tokenizer.pad_token = tokenizer.eos_token

def tokenization_process(input_data):
  input_data.pop("article", None)
  input_data.pop("abstract", None)
  inputs = tokenizer(input_data['text'], max_length=max_seq_len, truncation=True, padding="max_length", return_tensors="pt")
  targets = tokenizer(input_data['summary'], max_length=max_seq_len, truncation=True, padding="max_length", return_tensors="pt")

  # For causal LM, concatenate the input and output for training
  input_ids = inputs['input_ids']
  labels = targets['input_ids']
  return {'input_ids': input_ids, 'attention_mask': inputs['attention_mask'], 'labels': labels}#'decoder_input_ids': labels, 'labels': labels}

"""# Quantization Helpers from QLoRA"""

import bitsandbytes
def find_all_linear_names(bits, model):
    cls = bitsandbytes.nn.Linear4bit if bits == 4 else (bitsandbytes.nn.Linear8bitLt if bits == 8 else torch.nn.Linear)
    lora_module_names = set()
    for name, module in model.named_modules():
        if isinstance(module, cls):
            names = name.split('.')
            lora_module_names.add(names[0] if len(names) == 1 else names[-1])


    if 'lm_head' in lora_module_names: # needed for 16-bit
        lora_module_names.remove('lm_head')
    return list(lora_module_names)

getattr(model, "quantization_method", None)

from peft import prepare_model_for_kbit_training

full_finetune = False

if not full_finetune:
  model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)

# disk_offload(model, dir+"saved_models") # can't save.
# for param in model.parameters():
#     param.requires_grad = True
# model.tie_weights()
# from accelerate import Accelerator
# model = Accelerator.prepare(model)

"""# Loading Dataset - Arxiv"""

# Dataset helper

# import os
import pandas as pd
from datasets import load_dataset, load_from_disk, DatasetDict, Dataset

def loading_dataset(dataset_name, force_run=False):
  local_path = dir + dataset_info_dict[dataset_name]["local_path"]
  if not os.path.exists(local_path) or force_run:
    dataset_id = dataset_info_dict[dataset_name]["dataset_id"]
    dataset = load_dataset(path=dataset_id, streaming=True, trust_remote_code=True)
    dataset_dict = {}
    for split, data in dataset.items():
        # if split == "train":
        data = list(data)[:500] # TODO: selecting random 1k or 5k or 10k
        df = pd.DataFrame(data)
        dataset_dict[split] = Dataset.from_pandas(df) # dataset[split].to_pandas())
    _dataset = DatasetDict(dataset_dict)
    _dataset.save_to_disk(local_path)
    return _dataset
  else:
    dataset = load_from_disk(local_path)
    return dataset

def preprocess_dataset(_data):
  return {"text": _data["article"], "summary": _data["abstract"]}

# dataset_arxiv = load_dataset(path='ccdv/arxiv-summarization', streaming=True, trust_remote_code=True)

# arxiv_dataset_id = "ccdv/arxiv-summarization"
# arxiv_local_dataset_path = "domains/arxiv_summarization"
# dataset_arxiv = load_dataset(path=arxiv_dataset_id, streaming=True, trust_remote_code=True)
# pubmed_dataset_id = "ccdv/pubmed-summarization"
# pubmed_local_dataset_path = "domains/pubmed_summarization"
# dataset_pubmed = load_dataset(path=pubmed_dataset_id, streaming=True, trust_remote_code=True)
import os
dataset_arxiv = loading_dataset("arxiv") #, force_run=True)
# dataset_pubmed = loading_dataset("pubmed")


# Encode the input data
dataset_arxiv = dataset_arxiv.map(preprocess_dataset, batched=True)
dataset_arxiv = dataset_arxiv.map(tokenization_process, batched=True)
print("Arxiv: ", dataset_arxiv)
# dataset_pubmed = dataset_pubmed.map(preprocess_dataset, batched=True)
# print("Pubmed: ", dataset_pubmed)

"""Errors faced:
```
bnb=True:
    ImportError: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`


bnb=False:
    Model = Mistral:
        Some parameters are on the meta device device because they were offloaded to the disk.
        NotImplementedError: Cannot copy out of meta tensor; no data!

    Model = Llama:
        ValueError: You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.
```

Also, no more memory!

# PEFT - LoRA (QLoRA)
"""

from peft import get_peft_model, LoraConfig, TaskType, PeftConfig, prepare_model_for_kbit_training

modules = find_all_linear_names(bits, model) # bits

lora_config = LoraConfig(
    r=8, # 16
    lora_alpha=16,
    target_modules=modules,
    lora_dropout=0.00,  # dropout probability for layers
    bias="none",
    task_type=TaskType.CAUSAL_LM #SEQ_2_SEQ_LM
)

# results = "results"
# peft_config = PeftConfig.from_pretrained(results)


# from da_llms.models.llama import llama_model
# llama_tokenizer, llama_llm = llama_model()
# trainer = pyreft.ReftTrainerForCausalLM(
#     model=llama_llm, tokenizer=llama_tokenizer, args=training_args, **data_module)
# _ = trainer.train()

# model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)

from peft.tuners.lora import LoraLayer

# for param in model.parameters():
#     param.requires_grad = True

for name, module in model.named_modules():
  if isinstance(module, LoraLayer):
    if bf16:
      module = module.to(torch.bfloat16)
  if 'norm' in name:
    module = module.to(torch.float16)
  if 'lm_head' in name or 'embed_tokens' in name:
    if hasattr(module, 'weight'):
      if bf16 and module.weight.dtype == torch.float16:
        module = module.to(torch.bfloat16)

print(model.print_trainable_parameters())

"""# Training on Arxiv Dataset"""

model.config.use_cache = False
print('loaded model')
set_seed(42)

from transformers import DataCollatorForSeq2Seq
import transformers, copy
from dataclasses import dataclass, field
from typing import Optional, Dict, Sequence
from torch.nn.utils.rnn import pad_sequence
# DataCollatorForLanguageModeling

IGNORE_INDEX = -100
DEFAULT_PAD_TOKEN = "[PAD]"


@dataclass
class DataCollatorForCausalLM(object):
    tokenizer: transformers.PreTrainedTokenizer
    source_max_len: int
    target_max_len: int
    train_on_source: bool
    predict_with_generate: bool

    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:
        # Extract elements
        sources = [f"{self.tokenizer.bos_token}{example['input_ids']}" for example in instances]
        targets = [f"{example['labels']}{self.tokenizer.eos_token}" for example in instances]
        # Tokenize
        tokenized_sources_with_prompt = self.tokenizer(
            sources,
            max_length=self.source_max_len,
            truncation=True,
            add_special_tokens=False,
        )
        tokenized_targets = self.tokenizer(
            targets,
            max_length=self.target_max_len,
            truncation=True,
            add_special_tokens=False,
        )
        # Build the input and labels for causal LM
        input_ids = []
        labels = []
        for tokenized_source, tokenized_target in zip(
            tokenized_sources_with_prompt['input_ids'],
            tokenized_targets['input_ids']
        ):
            if not self.predict_with_generate:
                input_ids.append(torch.tensor(tokenized_source + tokenized_target))
                if not self.train_on_source:
                    labels.append(
                        torch.tensor([IGNORE_INDEX for _ in range(len(tokenized_source))] + copy.deepcopy(tokenized_target))
                    )
                else:
                    labels.append(torch.tensor(copy.deepcopy(tokenized_source + tokenized_target)))
            else:
                input_ids.append(torch.tensor(tokenized_source))
        # Apply padding
        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)
        labels = pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX) if not self.predict_with_generate else None
        data_dict = {
            'input_ids': input_ids,
            'attention_mask':input_ids.ne(self.tokenizer.pad_token_id),
        }
        if labels is not None:
            data_dict['labels'] = labels
        return data_dict

# data_collator = DataCollatorForCausalLM(
#         tokenizer=tokenizer,
#         source_max_len=max_seq_len,
#         target_max_len=max_seq_len,
#         train_on_source=True, # False
#         predict_with_generate=False,
#     )

data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding="max_length", max_length=max_seq_len, label_pad_token_id=-100)

tr_arxiv = dataset_arxiv["train"]    #  [:500]
val_arxiv = dataset_arxiv["validation"]  # [:500]

print(val_arxiv)

from transformers import TrainingArguments, Seq2SeqTrainingArguments, Seq2SeqTrainer, Trainer


training_args = Seq2SeqTrainingArguments( # TrainingArguments
    output_dir=dir+'results',
    evaluation_strategy="epoch",
    learning_rate=2e-4,
    # gradient_checkpointing=True,
    optim="adamw_hf",
    per_device_train_batch_size=1,
    per_device_eval_batch_size=1,
    gradient_accumulation_steps=16,
    num_train_epochs=1,
    weight_decay=0.01,
    bf16=bf16,
    fp16=True,
    tf32=False,
    logging_dir= dir+"logs",
    logging_steps=1,
    lr_scheduler_type="constant",
    disable_tqdm=False
    # save_total_limit=1,
    # predict_with_generate=True
)

from accelerate import DataLoaderConfiguration
from trl import SFTTrainer

# max_seq_length = 2048
# trainer = SFTTrainer(
#     model=model,
#     train_dataset=tr_arxiv,
#     eval_dataset=val_arxiv,
#     peft_config=lora_config,
#     data_collator=data_collator,
#     max_seq_length=max_seq_len,
#     tokenizer=tokenizer,
#     packing=True,
#     args=training_args
# )

# trainer = Trainer(
#     model=model,
#     args=training_args,
#     train_dataset=tr_arxiv,
#     eval_dataset=val_arxiv,
#     tokenizer=tokenizer,
#     # max_sequence_len=max_seq_len
#     # dataloader_config=dataloader_config
#     # peft_config=lora_config
# )


trainer = Seq2SeqTrainer(
    model=model,
    args=training_args,
    train_dataset=tr_arxiv,
    eval_dataset=val_arxiv,
    tokenizer=tokenizer,
    data_collator=data_collator
)

from transformers import TrainerCallback
trainer.add_callback(TrainerCallback)

trainer = accelerator.prepare(trainer)

tr_results = trainer.train()

"""without k_bit => OutOfMemoryError: CUDA out of memory. Tried to allocate 172.00 MiB. GPU   




with kbit => RuntimeError: CUDA error: device-side assert triggered Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.
"""

!nvcc

os.getenv("TORCH_USE_CUDA_DSA")

results = trainer.evaluate()
print(results)

# new_articles = ["New article text here..."]

gen_art = dataset_arxiv["test"]

i = 1
texts ,sums = [], []
for k, t in enumerate(gen_art):
  if k < i:
    texts.append(t["text"])
    sums.append(t["summary"])



inputs = tokenizer(texts, return_tensors="pt", max_length=max_seq_len, truncation=True, padding="max_length")
inputs = inputs.to(model.device)
outputs = model.generate(inputs.input_ids, max_length=1024, num_beams=5, early_stopping=True)
summaries = [tokenizer.decode(output, skip_special_tokens=True) for output in outputs]
for s1, s2 in zip(sums, summaries):
    print("actual: ", s1)
    print("pred: ", s2)