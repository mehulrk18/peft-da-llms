{
  "cells": [
    {
      "source": [
        "# !pip uninstall torch torchvision torchaudio -y"
      ],
      "cell_type": "code",
      "metadata": {
        "id": "7E14WccB5P_4"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 --extra-index-url https://download.pytorch.org/whl/nightly/cu118"
      ],
      "metadata": {
        "id": "tiiym19g5V6O"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1U5kQ3Mk8Ahr",
        "outputId": "97b1138c-8294-42ef-caaa-223cabc2c6bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['TORCH_USE_CUDA_DSA'] = '1'\n",
        "\n",
        "print(\"Environment variable TORCH_USE_CUDA_DSA set. Please restart the runtime.\")"
      ],
      "metadata": {
        "id": "dmqKT5G5urZe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5c8c082-78e1-440a-a523-27a674b784fa"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Environment variable TORCH_USE_CUDA_DSA set. Please restart the runtime.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.getenv(\"TORCH_USE_CUDA_DSA\")"
      ],
      "metadata": {
        "id": "7ct7BN9gu0Y8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "fff54e29-1405-42fe-dc6b-cf826c5b8dce"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = '1'\n",
        "os.getenv(\"CUDA_LAUNCH_BLOCKING\")\n",
        "# %env TORCH_USE_CUDA_DSA=1"
      ],
      "metadata": {
        "id": "RuCWT2PlsurG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b0ed8940-1964-4036-8f84-0f73e8e86fc5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SdR_TAlQnLrq"
      },
      "outputs": [],
      "source": [
        "# !pip install torch\n",
        "# !pip install fsspec==2024.2.0\n",
        "# !pip install datasets==2.18.0\n",
        "# !pip install transformers==4.39.3\n",
        "# !pip install bitsandbytes==0.42.0\n",
        "# !pip install huggingface-hub==0.23.4\n",
        "# !pip install accelerate==0.30.0\n",
        "# !pip install peft\n",
        "# !pip install trl\n",
        "# !pip install ninja packaging\n",
        "# # !MAX_JOBS=2 pip install flash-attn --no-build-isolation\n",
        "# !pip install ninja packaging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "zNdrbG6tnKoT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "GqOPePwoxLNJ"
      },
      "outputs": [],
      "source": [
        "from accelerate import Accelerator, init_empty_weights, load_checkpoint_and_dispatch\n",
        "accelerator = Accelerator()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "6VlGceuAnKoU"
      },
      "outputs": [],
      "source": [
        "# !pip show huggingface_hub\n",
        "# !pip show accelerate"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Configs"
      ],
      "metadata": {
        "id": "dxHLWu55h1iY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "Z9JuDle85MPI"
      },
      "outputs": [],
      "source": [
        "dataset_info_dict = {\n",
        "    \"arxiv\": {\n",
        "        \"dataset_id\": \"ccdv/arxiv-summarization\",\n",
        "        \"local_path\": \"domains/arxiv_summarization\"\n",
        "    },\n",
        "    \"pubmed\": {\n",
        "        \"dataset_id\": \"ccdv/pubmed-summarization\",\n",
        "        \"local_path\": \"domains/pubmed_summarization\"\n",
        "    }\n",
        "}\n",
        "\n",
        "dir = \"/content/drive/My Drive/Colab Notebooks/\"\n",
        "max_seq_len = 256 # context window # 1024\n",
        "ddtype = torch.float16 # float16\n",
        "bits = 4 #8\n",
        "bf16 = False\n",
        "bf32 = True\n",
        "fp16 = False\n",
        "compute_dtype = ddtype# torch.float32 # torch.bfloat16 if bf16 else torch.float32\n",
        "\n",
        "# Model Config\n",
        "llama3_model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
        "mistral_model_id = \"mistralai/Mistral-7B-v0.3\" # \"mistralai/Mixtral-8x7B-v0.1\"  # \"meta-llama/Meta-Llama-3-8B-Instruct\" #  \"meta-llama/Llama-2-7b-hf\" # \"TheBloke/Llama-2-7B-Chat-GGML\"\n",
        "llama2 = \"meta-llama/Llama-2-7b-hf\"\n",
        "dgpt = \"distilgpt2\"\n",
        "model_id = mistral_model_id # llama2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I0UDCy9BnKoY",
        "outputId": "6392137c-77c2-42d2-c6aa-8a4a1531f252"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "device:  cuda\n",
            "gpus->  1\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "# device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
        "print(\"device: \", device)\n",
        "\n",
        "if device == \"cuda\":\n",
        "  n_gpus = torch.cuda.device_count()\n",
        "  print(\"gpus-> \", n_gpus)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "login(token=\"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\")"
      ],
      "metadata": {
        "id": "FPUnDy44eY1d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94f971bb-61f9-4b3c-f8a3-00c093da2bf1"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "h_P7GOi5nKoY"
      },
      "source": [
        "# LLama3 Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239,
          "referenced_widgets": [
            "c1cfae8c916344e0a644c290c17b00e3",
            "206f2080d90d4f959e00f73ef6db718b",
            "861f4600553e4ada88d516ebe82f72f6",
            "f81029cb9be940bc920e42fb3b2a5a7c",
            "9d45dc33e7594071b3672fd9ec588570",
            "3f68adad70f147d490b460b4414c4f18",
            "854d683b2dba4dd0a34147d217d0cbcd",
            "5adae82b5a554323b84b886c334c1daa",
            "1a6cbad58021463280513d1f252e9fe5",
            "75f6483dd78a4ebfaa3bda30f2ba5272",
            "4b1cee273b4d4c3d82f6e21f2ed8d520"
          ]
        },
        "collapsed": true,
        "id": "jZrflzxdnKoY",
        "outputId": "f2de95ea-0db7-4e89-d988-55188c284ce5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:468: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c1cfae8c916344e0a644c290c17b00e3"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from accelerate import disk_offload\n",
        "from transformers import BitsAndBytesConfig, AutoModelForCausalLM, AutoModelForSeq2SeqLM, set_seed\n",
        "\n",
        "add_bnb_config = True\n",
        "use_flash_attn = True\n",
        "if add_bnb_config:\n",
        "  model = AutoModelForCausalLM.from_pretrained(\n",
        "      model_id,\n",
        "      # cache_dir=args.cache_dir,\n",
        "      # load_in_4bit=False,\n",
        "      # load_in_8bit=True,\n",
        "      device_map=\"auto\",\n",
        "      # max_memory=max_memory,\n",
        "      quantization_config=BitsAndBytesConfig(\n",
        "          load_in_4bit=bits == 4,\n",
        "          load_in_8bit=bits == 8,\n",
        "          llm_int8_threshold=6.0,\n",
        "          llm_int8_has_fp16_weight=True,\n",
        "          bnb_4bit_compute_dtype=compute_dtype,\n",
        "          bnb_4bit_use_double_quant=True,\n",
        "          bnb_4bit_quant_type=\"nf4\",\n",
        "      ),\n",
        "      torch_dtype=ddtype, #(torch.float32 if args.fp16 else (torch.bfloat16 if args.bf16 else torch.float32)),\n",
        "      trust_remote_code=True,\n",
        "      use_auth_token=True\n",
        "  )\n",
        "\n",
        "  setattr(model, 'model_parallel', True)\n",
        "  setattr(model, 'is_parallelizable', True)\n",
        "\n",
        "  model.config.torch_dtype= ddtype # (torch.float32 if args.fp16 else (torch.bfloat16 if args.bf16 else torch.float32))\n",
        "    # bnb_config = BitsAndBytesConfig(\n",
        "    #     # load_in_4bit=True,\n",
        "    #     load\n",
        "    #     bnb_4bit_use_double_quant=True,\n",
        "    #     bnb_4bit_quant_type=\"nf4\",\n",
        "    #     bnb_4bit_compute_dtype=torch.float32,# torch.bfloat16,\n",
        "    #     llm_int8_threshold=6.0,\n",
        "    #     llm_int8_has_fp16_weight=True, # False\n",
        "\n",
        "\n",
        "\n",
        "    #     # llm_int8_threshold=6.0,\n",
        "\n",
        "    #     # load_in_8bit=True,\n",
        "    #     # bnb_8bit_use_double_quant=True,\n",
        "    #     # bnb_8bit_quant_type=\"nf4\",\n",
        "    #     # bnb_8bit_compute_dtype=torch.bfloat16,\n",
        "    #     # llm_int8_has_fp16_weight = True\n",
        "\n",
        "\n",
        "    #     # load_in_8bit_fp32_cpu_offload=True\n",
        "    #     # bnb_8bit_use_double_quant=True,\n",
        "    #     # bnb_8bit_quant_type=\"nf4\",\n",
        "    #     # bnb_8bit_compute_dtype=torch.bfloat16\n",
        "    # )\n",
        "\n",
        "    # model = AutoModelForCausalLM.from_pretrained(\n",
        "    #     model_id,\n",
        "    #     device_map=\"auto\",\n",
        "    #     # load_in_4bit=True,\n",
        "    #     torch_dtype=torch.float32, #torch.bfloat16\n",
        "    #     # use_flash_attention_2=use_flash_attn,\n",
        "    #     # attn_implementation=\"flash_attention_2\",\n",
        "    #     low_cpu_mem_usage=True,\n",
        "    #     trust_remote_code=True,\n",
        "    #     # max_memory={0: \"15GB\"}, # Uncomment this line with you encounter CUDA out of memory errors\n",
        "    #     quantization_config=bnb_config\n",
        "    # )\n",
        "    # disk_offload(model, dir+\"saved_models\")\n",
        "\n",
        "else:\n",
        "  # with init_empty_weights():\n",
        "  model =  AutoModelForCausalLM.from_pretrained(  # AutoModelForSeq2SeqLM.from_pretrained(\n",
        "      model_id,\n",
        "      # device_map=\"auto\",\n",
        "      torch_dtype=torch.float32,\n",
        "      low_cpu_mem_usage=True,\n",
        "      trust_remote_code=True,\n",
        "      # offload_buffers=True,\n",
        "      # max_memory={0: \"15GB\"} # Uncomment this line with you encounter CUDA out of memory errors\n",
        "  ) # .config\n",
        "\n",
        "  # with init_empty_weights():\n",
        "  #   model = AutoModelForCausalLM.from_config(model_config) #, device_map=\"auto\")\n",
        "\n",
        "  offload_folder = dir+\"saved_models\"\n",
        "  # model = load_checkpoint_and_dispatch(model, model_id, device_map=\"auto\")\n",
        "\n",
        "  # for name, module in model.named_modules():\n",
        "  #   if isinstance(module, torch.nn.Module):\n",
        "  #       disk_offload(module, offload_folder)\n",
        "  # for layer in model.transformer.h:\n",
        "  #   disk_offload(layer, offload_folder)\n",
        "\n",
        "  model = model.to(device)\n",
        "    # model.disk_offload(device)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenizer"
      ],
      "metadata": {
        "id": "UvgB3Gvvj_ug"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"right\", tokenizer_type='llama' if 'llama' in model_id else None, trust_remote_code=True, use_fast=True)\n",
        "tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "model.config.torch_dtype=torch.float32# (torch.float16 if fp16 else (torch.bfloat16 if bf16 else torch.float16))\n",
        "if 'llama' in model_id:\n",
        "  tokenizer.add_special_tokens({\n",
        "              \"eos_token\": tokenizer.convert_ids_to_tokens(model.config.eos_token_id),\n",
        "              \"bos_token\": tokenizer.convert_ids_to_tokens(model.config.bos_token_id),\n",
        "              # \"unk_token\": tokenizer.convert_ids_to_tokens(\n",
        "              #     model.config.pad_token_id if model.config.pad_token_id != -1 else tokenizer.pad_token_id\n",
        "              # ),\n",
        "      })\n",
        "else:\n",
        "  tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "NYFP8xyMeQbH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da12fc85-b3f3-4281-f913-72f45de69575"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenization_process(input_data):\n",
        "  input_data.pop(\"article\", None)\n",
        "  input_data.pop(\"abstract\", None)\n",
        "  inputs = tokenizer(input_data['text'], max_length=max_seq_len, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "  targets = tokenizer(input_data['summary'], max_length=max_seq_len, truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n",
        "\n",
        "  # For causal LM, concatenate the input and output for training\n",
        "  input_ids = inputs['input_ids']\n",
        "  labels = targets['input_ids']\n",
        "  return {'input_ids': input_ids, 'attention_mask': inputs['attention_mask'], 'labels': labels}#'decoder_input_ids': labels, 'labels': labels}\n"
      ],
      "metadata": {
        "id": "8XHkKv-Vgtvw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization Helpers from QLoRA"
      ],
      "metadata": {
        "id": "bM4JgXJDdcKG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import bitsandbytes\n",
        "def find_all_linear_names(bits, model):\n",
        "    cls = bitsandbytes.nn.Linear4bit if bits == 4 else (bitsandbytes.nn.Linear8bitLt if bits == 8 else torch.nn.Linear)\n",
        "    lora_module_names = set()\n",
        "    for name, module in model.named_modules():\n",
        "        if isinstance(module, cls):\n",
        "            names = name.split('.')\n",
        "            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n",
        "\n",
        "\n",
        "    if 'lm_head' in lora_module_names: # needed for 16-bit\n",
        "        lora_module_names.remove('lm_head')\n",
        "    return list(lora_module_names)"
      ],
      "metadata": {
        "id": "kSqmaUiJdfd7"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "getattr(model, \"quantization_method\", None)"
      ],
      "metadata": {
        "id": "ZMV7AgRxgxGi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "4625a818-b4a3-4ddb-ebd0-571f7d11e7cd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<QuantizationMethod.BITS_AND_BYTES: 'bitsandbytes'>"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "DQ5Yo7gntwo1"
      },
      "outputs": [],
      "source": [
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "full_finetune = False\n",
        "\n",
        "if not full_finetune:\n",
        "  model = prepare_model_for_kbit_training(model, use_gradient_checkpointing=True)\n",
        "\n",
        "# disk_offload(model, dir+\"saved_models\") # can't save.\n",
        "# for param in model.parameters():\n",
        "#     param.requires_grad = True\n",
        "# model.tie_weights()\n",
        "# from accelerate import Accelerator\n",
        "# model = Accelerator.prepare(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "E-oLOpOjnKoQ"
      },
      "source": [
        "# Loading Dataset - Arxiv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "mvsskH_XnKoX"
      },
      "outputs": [],
      "source": [
        "# Dataset helper\n",
        "\n",
        "# import os\n",
        "import pandas as pd\n",
        "from datasets import load_dataset, load_from_disk, DatasetDict, Dataset\n",
        "\n",
        "def loading_dataset(dataset_name, force_run=False):\n",
        "  local_path = dir + dataset_info_dict[dataset_name][\"local_path\"]\n",
        "  if not os.path.exists(local_path) or force_run:\n",
        "    dataset_id = dataset_info_dict[dataset_name][\"dataset_id\"]\n",
        "    dataset = load_dataset(path=dataset_id, streaming=True, trust_remote_code=True)\n",
        "    dataset_dict = {}\n",
        "    for split, data in dataset.items():\n",
        "        # if split == \"train\":\n",
        "        data = list(data)[:500] # TODO: selecting random 1k or 5k or 10k\n",
        "        df = pd.DataFrame(data)\n",
        "        dataset_dict[split] = Dataset.from_pandas(df) # dataset[split].to_pandas())\n",
        "    _dataset = DatasetDict(dataset_dict)\n",
        "    _dataset.save_to_disk(local_path)\n",
        "    return _dataset\n",
        "  else:\n",
        "    dataset = load_from_disk(local_path)\n",
        "    return dataset\n",
        "\n",
        "def preprocess_dataset(_data):\n",
        "  return {\"text\": _data[\"article\"], \"summary\": _data[\"abstract\"]}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "A_PRCiYh6wst",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "913df07f-8528-4dfb-e9dd-3969e4c7c332"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Arxiv:  DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "    validation: Dataset({\n",
            "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
            "        num_rows: 500\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "# dataset_arxiv = load_dataset(path='ccdv/arxiv-summarization', streaming=True, trust_remote_code=True)\n",
        "\n",
        "# arxiv_dataset_id = \"ccdv/arxiv-summarization\"\n",
        "# arxiv_local_dataset_path = \"domains/arxiv_summarization\"\n",
        "# dataset_arxiv = load_dataset(path=arxiv_dataset_id, streaming=True, trust_remote_code=True)\n",
        "# pubmed_dataset_id = \"ccdv/pubmed-summarization\"\n",
        "# pubmed_local_dataset_path = \"domains/pubmed_summarization\"\n",
        "# dataset_pubmed = load_dataset(path=pubmed_dataset_id, streaming=True, trust_remote_code=True)\n",
        "import os\n",
        "dataset_arxiv = loading_dataset(\"arxiv\") #, force_run=True)\n",
        "# dataset_pubmed = loading_dataset(\"pubmed\")\n",
        "\n",
        "\n",
        "# Encode the input data\n",
        "dataset_arxiv = dataset_arxiv.map(preprocess_dataset, batched=True)\n",
        "dataset_arxiv = dataset_arxiv.map(tokenization_process, batched=True)\n",
        "print(\"Arxiv: \", dataset_arxiv)\n",
        "# dataset_pubmed = dataset_pubmed.map(preprocess_dataset, batched=True)\n",
        "# print(\"Pubmed: \", dataset_pubmed)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "GtV9hEhMnKoY"
      },
      "source": [
        "Errors faced:\n",
        "```\n",
        "bnb=True:\n",
        "    ImportError: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\n",
        "\n",
        "\n",
        "bnb=False:\n",
        "    Model = Mistral:\n",
        "        Some parameters are on the meta device device because they were offloaded to the disk.\n",
        "        NotImplementedError: Cannot copy out of meta tensor; no data!\n",
        "\n",
        "    Model = Llama:\n",
        "        ValueError: You are trying to offload the whole model to the disk. Please use the `disk_offload` function instead.\n",
        "```\n",
        "\n",
        "Also, no more memory!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": false,
        "id": "ULqhe4YYnKoY"
      },
      "source": [
        "# PEFT - LoRA (QLoRA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "dO4zjusKnKoY"
      },
      "outputs": [],
      "source": [
        "from peft import get_peft_model, LoraConfig, TaskType, PeftConfig, prepare_model_for_kbit_training\n",
        "\n",
        "modules = find_all_linear_names(bits, model) # bits\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8, # 16\n",
        "    lora_alpha=16,\n",
        "    target_modules=modules,\n",
        "    lora_dropout=0.00,  # dropout probability for layers\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.CAUSAL_LM #SEQ_2_SEQ_LM\n",
        ")\n",
        "\n",
        "# results = \"results\"\n",
        "# peft_config = PeftConfig.from_pretrained(results)\n",
        "\n",
        "\n",
        "# from da_llms.models.llama import llama_model\n",
        "# llama_tokenizer, llama_llm = llama_model()\n",
        "# trainer = pyreft.ReftTrainerForCausalLM(\n",
        "#     model=llama_llm, tokenizer=llama_tokenizer, args=training_args, **data_module)\n",
        "# _ = trainer.train()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "LLGP3wd7u37d"
      },
      "outputs": [],
      "source": [
        "# model = prepare_model_for_kbit_training(model)\n",
        "model = get_peft_model(model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from peft.tuners.lora import LoraLayer\n",
        "\n",
        "# for param in model.parameters():\n",
        "#     param.requires_grad = True\n",
        "\n",
        "for name, module in model.named_modules():\n",
        "  if isinstance(module, LoraLayer):\n",
        "    if bf16:\n",
        "      module = module.to(torch.bfloat16)\n",
        "  if 'norm' in name:\n",
        "    module = module.to(torch.float16)\n",
        "  if 'lm_head' in name or 'embed_tokens' in name:\n",
        "    if hasattr(module, 'weight'):\n",
        "      if bf16 and module.weight.dtype == torch.float16:\n",
        "        module = module.to(torch.bfloat16)"
      ],
      "metadata": {
        "id": "FHup6Llb4B2d"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KP6GBoFX8JlN",
        "outputId": "9f85d789-8f30-4e76-ee8b-88bdf4e9ec3d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 20,971,520 || all params: 7,268,995,072 || trainable%: 0.2885\n",
            "None\n"
          ]
        }
      ],
      "source": [
        "print(model.print_trainable_parameters())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFRlqF2Xw0-5"
      },
      "source": [
        "# Training on Arxiv Dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.use_cache = False\n",
        "print('loaded model')\n",
        "set_seed(42)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyM88gMYk_Nu",
        "outputId": "9adc66df-2344-4186-84ad-94d722e5666e"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loaded model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq\n",
        "import transformers, copy\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional, Dict, Sequence\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "# DataCollatorForLanguageModeling\n",
        "\n",
        "IGNORE_INDEX = -100\n",
        "DEFAULT_PAD_TOKEN = \"[PAD]\"\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DataCollatorForCausalLM(object):\n",
        "    tokenizer: transformers.PreTrainedTokenizer\n",
        "    source_max_len: int\n",
        "    target_max_len: int\n",
        "    train_on_source: bool\n",
        "    predict_with_generate: bool\n",
        "\n",
        "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
        "        # Extract elements\n",
        "        sources = [f\"{self.tokenizer.bos_token}{example['input_ids']}\" for example in instances]\n",
        "        targets = [f\"{example['labels']}{self.tokenizer.eos_token}\" for example in instances]\n",
        "        # Tokenize\n",
        "        tokenized_sources_with_prompt = self.tokenizer(\n",
        "            sources,\n",
        "            max_length=self.source_max_len,\n",
        "            truncation=True,\n",
        "            add_special_tokens=False,\n",
        "        )\n",
        "        tokenized_targets = self.tokenizer(\n",
        "            targets,\n",
        "            max_length=self.target_max_len,\n",
        "            truncation=True,\n",
        "            add_special_tokens=False,\n",
        "        )\n",
        "        # Build the input and labels for causal LM\n",
        "        input_ids = []\n",
        "        labels = []\n",
        "        for tokenized_source, tokenized_target in zip(\n",
        "            tokenized_sources_with_prompt['input_ids'],\n",
        "            tokenized_targets['input_ids']\n",
        "        ):\n",
        "            if not self.predict_with_generate:\n",
        "                input_ids.append(torch.tensor(tokenized_source + tokenized_target))\n",
        "                if not self.train_on_source:\n",
        "                    labels.append(\n",
        "                        torch.tensor([IGNORE_INDEX for _ in range(len(tokenized_source))] + copy.deepcopy(tokenized_target))\n",
        "                    )\n",
        "                else:\n",
        "                    labels.append(torch.tensor(copy.deepcopy(tokenized_source + tokenized_target)))\n",
        "            else:\n",
        "                input_ids.append(torch.tensor(tokenized_source))\n",
        "        # Apply padding\n",
        "        input_ids = pad_sequence(input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id)\n",
        "        labels = pad_sequence(labels, batch_first=True, padding_value=IGNORE_INDEX) if not self.predict_with_generate else None\n",
        "        data_dict = {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask':input_ids.ne(self.tokenizer.pad_token_id),\n",
        "        }\n",
        "        if labels is not None:\n",
        "            data_dict['labels'] = labels\n",
        "        return data_dict\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xiq9IN7Uinah"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data_collator = DataCollatorForCausalLM(\n",
        "#         tokenizer=tokenizer,\n",
        "#         source_max_len=max_seq_len,\n",
        "#         target_max_len=max_seq_len,\n",
        "#         train_on_source=True, # False\n",
        "#         predict_with_generate=False,\n",
        "#     )\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model, padding=\"max_length\", max_length=max_seq_len, label_pad_token_id=-100)"
      ],
      "metadata": {
        "id": "2gPIxJUoe37P"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "GR6Fwll8w7FQ"
      },
      "outputs": [],
      "source": [
        "tr_arxiv = dataset_arxiv[\"train\"]    #  [:500]\n",
        "val_arxiv = dataset_arxiv[\"validation\"]  # [:500]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tWTB0e5HGr-w",
        "outputId": "07b34792-bd39-4d28-eda9-12e5c7d81eb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['text', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
            "    num_rows: 500\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(val_arxiv)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "6pW8oJvpxsdQ"
      },
      "outputs": [],
      "source": [
        "from transformers import TrainingArguments, Seq2SeqTrainingArguments, Seq2SeqTrainer, Trainer\n",
        "\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments( # TrainingArguments\n",
        "    output_dir=dir+'results',\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    learning_rate=2e-4,\n",
        "    # gradient_checkpointing=True,\n",
        "    optim=\"adamw_hf\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=16,\n",
        "    num_train_epochs=1,\n",
        "    weight_decay=0.01,\n",
        "    bf16=bf16,\n",
        "    fp16=True,\n",
        "    tf32=False,\n",
        "    logging_dir= dir+\"logs\",\n",
        "    logging_steps=1,\n",
        "    lr_scheduler_type=\"constant\",\n",
        "    disable_tqdm=False\n",
        "    # save_total_limit=1,\n",
        "    # predict_with_generate=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5k1wLBIIyTgs",
        "outputId": "838ea282-501e-4c0d-af5c-75de48b4a5b0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:446: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from accelerate import DataLoaderConfiguration\n",
        "from trl import SFTTrainer\n",
        "\n",
        "# max_seq_length = 2048\n",
        "# trainer = SFTTrainer(\n",
        "#     model=model,\n",
        "#     train_dataset=tr_arxiv,\n",
        "#     eval_dataset=val_arxiv,\n",
        "#     peft_config=lora_config,\n",
        "#     data_collator=data_collator,\n",
        "#     max_seq_length=max_seq_len,\n",
        "#     tokenizer=tokenizer,\n",
        "#     packing=True,\n",
        "#     args=training_args\n",
        "# )\n",
        "\n",
        "# trainer = Trainer(\n",
        "#     model=model,\n",
        "#     args=training_args,\n",
        "#     train_dataset=tr_arxiv,\n",
        "#     eval_dataset=val_arxiv,\n",
        "#     tokenizer=tokenizer,\n",
        "#     # max_sequence_len=max_seq_len\n",
        "#     # dataloader_config=dataloader_config\n",
        "#     # peft_config=lora_config\n",
        "# )\n",
        "\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tr_arxiv,\n",
        "    eval_dataset=val_arxiv,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainerCallback\n",
        "trainer.add_callback(TrainerCallback)"
      ],
      "metadata": {
        "id": "Jx1oAhIv4ltS"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "XLOnQ90CCUNw"
      },
      "outputs": [],
      "source": [
        "trainer = accelerator.prepare(trainer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "id": "tuFyTqV6CYYa",
        "outputId": "7043dacf-6b2a-496c-a93f-86fb558db9fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:457: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9' max='31' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 9/31 04:51 < 15:15, 0.02 it/s, Epoch 0.26/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "tr_results = trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "without k_bit => OutOfMemoryError: CUDA out of memory. Tried to allocate 172.00 MiB. GPU   \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "with kbit => RuntimeError: CUDA error: device-side assert triggered Compile with `TORCH_USE_CUDA_DSA` to enable device-side assertions."
      ],
      "metadata": {
        "id": "ud7puJocdUgo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!nvcc"
      ],
      "metadata": {
        "id": "pcBG4NK9MPDP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.getenv(\"TORCH_USE_CUDA_DSA\")"
      ],
      "metadata": {
        "id": "bQm2COK-Amqk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qNFZcR3OF4Tj"
      },
      "outputs": [],
      "source": [
        "results = trainer.evaluate()\n",
        "print(results)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c1cfae8c916344e0a644c290c17b00e3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_206f2080d90d4f959e00f73ef6db718b",
              "IPY_MODEL_861f4600553e4ada88d516ebe82f72f6",
              "IPY_MODEL_f81029cb9be940bc920e42fb3b2a5a7c"
            ],
            "layout": "IPY_MODEL_9d45dc33e7594071b3672fd9ec588570"
          }
        },
        "206f2080d90d4f959e00f73ef6db718b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f68adad70f147d490b460b4414c4f18",
            "placeholder": "​",
            "style": "IPY_MODEL_854d683b2dba4dd0a34147d217d0cbcd",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "861f4600553e4ada88d516ebe82f72f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5adae82b5a554323b84b886c334c1daa",
            "max": 3,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1a6cbad58021463280513d1f252e9fe5",
            "value": 3
          }
        },
        "f81029cb9be940bc920e42fb3b2a5a7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75f6483dd78a4ebfaa3bda30f2ba5272",
            "placeholder": "​",
            "style": "IPY_MODEL_4b1cee273b4d4c3d82f6e21f2ed8d520",
            "value": " 3/3 [01:25&lt;00:00, 27.67s/it]"
          }
        },
        "9d45dc33e7594071b3672fd9ec588570": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f68adad70f147d490b460b4414c4f18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "854d683b2dba4dd0a34147d217d0cbcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5adae82b5a554323b84b886c334c1daa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a6cbad58021463280513d1f252e9fe5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "75f6483dd78a4ebfaa3bda30f2ba5272": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b1cee273b4d4c3d82f6e21f2ed8d520": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}