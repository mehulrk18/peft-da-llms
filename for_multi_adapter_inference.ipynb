{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================BUG REPORT===================================\n",
      "Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      " and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues\n",
      "================================================================================\n",
      "bin /Users/mehul/TUM/SS24/DomainAdaptationLLMs/311_venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so\n",
      "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n",
      "CUDA SETUP: Loading binary /Users/mehul/TUM/SS24/DomainAdaptationLLMs/311_venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so...\n",
      "dlopen(/Users/mehul/TUM/SS24/DomainAdaptationLLMs/311_venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so, 0x0006): tried: '/Users/mehul/TUM/SS24/DomainAdaptationLLMs/311_venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file), '/System/Volumes/Preboot/Cryptexes/OS/Users/mehul/TUM/SS24/DomainAdaptationLLMs/311_venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (no such file), '/Users/mehul/TUM/SS24/DomainAdaptationLLMs/311_venv/lib/python3.11/site-packages/bitsandbytes/libbitsandbytes_cpu.so' (not a mach-o file)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/TUM/SS24/DomainAdaptationLLMs/311_venv/lib/python3.11/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "\n",
    "import adapters\n",
    "import pandas as pd\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "from dataset_lib import SumDataLoader\n",
    "from utils import generate_summary, get_pretrained_model, MODEL_ID, rouge_metric, LLaMAModelClass, \\\n",
    "    convert_params_to_bfloat16\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_path = \"saved_models/medical_lora_hf/medical_lora\"\n",
    "ia3_path = \"saved_models/medical_ia3_hf/medical_ia3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = \"medical\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LLaMA's: TinyLlama/TinyLlama-1.1B-Chat-v1.0 Model for the process.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mehul/TUM/SS24/DomainAdaptationLLMs/311_venv/lib/python3.11/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Model Loaded ***\n",
      "Loading LLaMA's: TinyLlama/TinyLlama-1.1B-Chat-v1.0 Tokenizer for the process.\n",
      "*** Tokenizer Loaded ***\n",
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "llama = LLaMAModelClass(version=3.0, instruct_mode=False, quantization_config=None, model_checkpoint=None, mlm=False)\n",
    "print(llama.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): lora.Linear(\n",
      "            (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (m_lora): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (m_lora): Linear(in_features=2048, out_features=8, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (m_lora): Linear(in_features=8, out_features=2048, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "            (lora_magnitude_vector): ModuleDict()\n",
      "          )\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): lora.Linear(\n",
      "            (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
      "            (lora_dropout): ModuleDict(\n",
      "              (m_lora): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (lora_A): ModuleDict(\n",
      "              (m_lora): Linear(in_features=2048, out_features=8, bias=False)\n",
      "            )\n",
      "            (lora_B): ModuleDict(\n",
      "              (m_lora): Linear(in_features=8, out_features=256, bias=False)\n",
      "            )\n",
      "            (lora_embedding_A): ParameterDict()\n",
      "            (lora_embedding_B): ParameterDict()\n",
      "            (lora_magnitude_vector): ModuleDict()\n",
      "          )\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from peft import PeftModel\n",
    "# llama.model = PeftModel.from_pretrained(llama.model, lora_path, adapter_name=\"medical_lora\")\n",
    "# print(llama.model)\n",
    "llama.model.load_adapter(lora_path, adapter_name=\"m_lora\")\n",
    "print(llama.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(\n",
      "            (base_layer): lora.Linear(\n",
      "              (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (m_lora): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (m_lora): Linear(in_features=2048, out_features=8, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (m_lora): Linear(in_features=8, out_features=2048, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (ia3_l): ParameterDict(  (m_ia3): Parameter containing: [torch.mps.FloatTensor of size 2048x1])\n",
      "          )\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear(\n",
      "            (base_layer): lora.Linear(\n",
      "              (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (m_lora): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (m_lora): Linear(in_features=2048, out_features=8, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (m_lora): Linear(in_features=8, out_features=256, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (ia3_l): ParameterDict(  (m_ia3): Parameter containing: [torch.mps.FloatTensor of size 256x1])\n",
      "          )\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): Linear(\n",
      "            (base_layer): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "            (ia3_l): ParameterDict(  (m_ia3): Parameter containing: [torch.mps.FloatTensor of size 1x5632])\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "llama.model.load_adapter(ia3_path, adapter_name=\"m_ia3\", offload_folder=\"offload_dir\")\n",
    "print(llama.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama.model.set_adapter([\"m_lora\", \"m_ia3\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(\n",
      "            (base_layer): lora.Linear(\n",
      "              (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (m_lora): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (m_lora): Linear(in_features=2048, out_features=8, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (m_lora): Linear(in_features=8, out_features=2048, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (ia3_l): ParameterDict(  (m_ia3): Parameter containing: [torch.mps.FloatTensor of size 2048x1])\n",
      "          )\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear(\n",
      "            (base_layer): lora.Linear(\n",
      "              (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (m_lora): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (m_lora): Linear(in_features=2048, out_features=8, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (m_lora): Linear(in_features=8, out_features=256, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (ia3_l): ParameterDict(  (m_ia3): Parameter containing: [torch.mps.FloatTensor of size 256x1])\n",
      "          )\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): Linear(\n",
      "            (base_layer): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "            (ia3_l): ParameterDict(  (m_ia3): Parameter containing: [torch.mps.FloatTensor of size 1x5632])\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(llama.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama.model.disable_adapters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m_lora', 'm_ia3']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama.model.active_adapters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LlamaForCausalLM(\n",
      "  (model): LlamaModel(\n",
      "    (embed_tokens): Embedding(32000, 2048)\n",
      "    (layers): ModuleList(\n",
      "      (0-21): 22 x LlamaDecoderLayer(\n",
      "        (self_attn): LlamaSdpaAttention(\n",
      "          (q_proj): Linear(\n",
      "            (base_layer): lora.Linear(\n",
      "              (base_layer): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (m_lora): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (m_lora): Linear(in_features=2048, out_features=8, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (m_lora): Linear(in_features=8, out_features=2048, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (ia3_l): ParameterDict(  (m_ia3): Parameter containing: [torch.mps.FloatTensor of size 2048x1])\n",
      "          )\n",
      "          (k_proj): Linear(in_features=2048, out_features=256, bias=False)\n",
      "          (v_proj): Linear(\n",
      "            (base_layer): lora.Linear(\n",
      "              (base_layer): Linear(in_features=2048, out_features=256, bias=False)\n",
      "              (lora_dropout): ModuleDict(\n",
      "                (m_lora): Dropout(p=0.1, inplace=False)\n",
      "              )\n",
      "              (lora_A): ModuleDict(\n",
      "                (m_lora): Linear(in_features=2048, out_features=8, bias=False)\n",
      "              )\n",
      "              (lora_B): ModuleDict(\n",
      "                (m_lora): Linear(in_features=8, out_features=256, bias=False)\n",
      "              )\n",
      "              (lora_embedding_A): ParameterDict()\n",
      "              (lora_embedding_B): ParameterDict()\n",
      "              (lora_magnitude_vector): ModuleDict()\n",
      "            )\n",
      "            (ia3_l): ParameterDict(  (m_ia3): Parameter containing: [torch.mps.FloatTensor of size 256x1])\n",
      "          )\n",
      "          (o_proj): Linear(in_features=2048, out_features=2048, bias=False)\n",
      "          (rotary_emb): LlamaRotaryEmbedding()\n",
      "        )\n",
      "        (mlp): LlamaMLP(\n",
      "          (gate_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (up_proj): Linear(in_features=2048, out_features=5632, bias=False)\n",
      "          (down_proj): Linear(\n",
      "            (base_layer): Linear(in_features=5632, out_features=2048, bias=False)\n",
      "            (ia3_l): ParameterDict(  (m_ia3): Parameter containing: [torch.mps.FloatTensor of size 1x5632])\n",
      "          )\n",
      "          (act_fn): SiLU()\n",
      "        )\n",
      "        (input_layernorm): LlamaRMSNorm()\n",
      "        (post_attention_layernorm): LlamaRMSNorm()\n",
      "      )\n",
      "    )\n",
      "    (norm): LlamaRMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=2048, out_features=32000, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(llama.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = \"\"\" Rome had begun expanding shortly after the founding of the Republic in the 6th century BC, though it did not expand outside the Italian Peninsula until the 3rd century BC, during the Punic Wars, afterwhich the Republic expanded across the Mediterranean.[5][6][7][8] Civil war engulfed Rome in the mid-1st century BC, first between Julius Caesar and Pompey, and finally between Octavian (Caesar's grand-nephew) and Mark Antony. Antony was defeated at the Battle of Actium in 31 BC, leading to the annexation of Egypt. In 27 BC, the Senate gave Octavian the titles of Augustus (\"venerated\") and Princeps (\"foremost\"), thus beginning the Principate, the first epoch of Roman imperial history. Augustus' name was inherited by his successors, as well as his title of Imperator (\"commander\"), from which the term \"emperor\" is derived. Early emperors avoided any association with the ancient kings of Rome, instead presenting themselves as leaders of the Republic.\\nThe success of Augustus in establishing principles of dynastic succession was limited by his outliving a number of talented potential heirs; the Julio-Claudian dynasty lasted for four more emperors—Tiberius, Caligula, Claudius, and Nero—before it yielded in AD 69 to the strife-torn Year of the Four Emperors, from which Vespasian emerged as victor. Vespasian became the founder of the brief Flavian dynasty, to be followed by the Nerva–Antonine dynasty which produced the \"Five Good Emperors\": Nerva, Trajan, Hadrian, Antoninus Pius and the philosophically inclined Marcus Aurelius. In the view of the Greek historian Cassius Dio, a contemporary observer, the accession of the emperor Commodus in AD 180 marked the descent \"from a kingdom of gold to one of rust and iron\"[9]—a famous comment which has led some historians, notably Edward Gibbon, to take Commodus' reign as the beginning of the decline of the Roman Empire. \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset_lib import llama3_testing_prompt, chat_template_prompt_inference\n",
    "terminators = [\n",
    "        llama.tokenizer.eos_token_id,\n",
    "        llama.tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "    ]\n",
    "content = llama3_testing_prompt(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nSummarize the given article by addressing the following key points:\\n\\n    1. Main Topic: What is the primary subject or theme of the article?\\n\\n    2. Context: What background information is provided to frame the discussion?\\n\\n    3. Key Arguments: What are the main arguments or points made by the author?\\n\\n    4. Evidence and Examples: What supporting evidence or examples are presented?\\n\\n    5. Conclusions: What conclusions does the author draw from the discussion?\\n\\n    6. Implications: What are the potential implications or significance of the article\\'s content for its audience?<|eot_id|>\\n\\n            <|start_header_id|>user<|end_header_id|>\\nPlease provide the summary for the article:\\n Rome had begun expanding shortly after the founding of the Republic in the 6th century BC, though it did not expand outside the Italian Peninsula until the 3rd century BC, during the Punic Wars, afterwhich the Republic expanded across the Mediterranean.[5][6][7][8] Civil war engulfed Rome in the mid-1st century BC, first between Julius Caesar and Pompey, and finally between Octavian (Caesar\\'s grand-nephew) and Mark Antony. Antony was defeated at the Battle of Actium in 31 BC, leading to the annexation of Egypt. In 27 BC, the Senate gave Octavian the titles of Augustus (\"venerated\") and Princeps (\"foremost\"), thus beginning the Principate, the first epoch of Roman imperial history. Augustus\\' name was inherited by his successors, as well as his title of Imperator (\"commander\"), from which the term \"emperor\" is derived. Early emperors avoided any association with the ancient kings of Rome, instead presenting themselves as leaders of the Republic.\\nThe success of Augustus in establishing principles of dynastic succession was limited by his outliving a number of talented potential heirs; the Julio-Claudian dynasty lasted for four more emperors—Tiberius, Caligula, Claudius, and Nero—before it yielded in AD 69 to the strife-torn Year of the Four Emperors, from which Vespasian emerged as victor. Vespasian became the founder of the brief Flavian dynasty, to be followed by the Nerva–Antonine dynasty which produced the \"Five Good Emperors\": Nerva, Trajan, Hadrian, Antoninus Pius and the philosophically inclined Marcus Aurelius. In the view of the Greek historian Cassius Dio, a contemporary observer, the accession of the emperor Commodus in AD 180 marked the descent \"from a kingdom of gold to one of rust and iron\"[9]—a famous comment which has led some historians, notably Edward Gibbon, to take Commodus\\' reign as the beginning of the decline of the Roman Empire. <|eot_id|>\\n\\n            <|start_header_id|>assistant<|end_header_id|>\\nHere is your Summary:'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"mps\" if torch.backends.mps.is_available else (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "inputs = llama.tokenizer(content, return_tensors=\"pt\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['m_lora', 'm_ia3']"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama.model.active_adapters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_len = len(inputs[\"input_ids\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    summary_ids = llama.model.generate(**inputs,\n",
    "                                 # max_length=512, \n",
    "                                 do_sample=True,  # Enable sampling\n",
    "                                 top_k=50,  # Top-k sampling\n",
    "                                 num_return_sequences=1,  # Generate a single sequence\n",
    "                                 eos_token_id=terminators,\n",
    "                                 # early_stopping=True,\n",
    "                                 temperature=0.6,\n",
    "                                 max_new_tokens=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_both = llama.tokenizer.decode(summary_ids[0][in_len:], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Rome had begun expanding shortly after the founding of the Republic in the 6th century BC, and expanded across the Mediterranean during the Punic Wars. Civil war engulfed Rome in the mid-1st century BC, leading to the annexation of Egypt. In 27 BC, the Senate gave Augustus the titles of Augustus and Princeps, and he began the Principate era. Augustus' successors established dynastic succession principles, but outlived a number of talented potential heirs. The Julio-Claudian dynasty lasted for four emperors, including Tiberius, Caligula, Claudius, and Nero. The Flavian dynasty was founded by Vespasian, followed by the Nerva–Antonine dynasty, which produced the \"Five Good Emperors\": Nerva, Trajan, Hadrian, Antoninus Pius, and Marcus Aurelius. The accession of Commodus marked the beginning of the decline of the Roman Empire.\n",
      "\n",
      "-> Rome had a long history of expansion and territorial conquests, beginning with the founding of the Republic in the 6th century BC and continuing through the Punic Wars and expansion across the Mediterranean. Civil war ensued after Julius Caesar's grand-nephew Octavian defeated Pompey and Mark Antony, leading to the annexation of Egypt. The Principate began with Augustus' ascension to the title of Imperator and his leadership of the Roman Empire. The success of Augustus in establishing dynastic succession was limited by his predecessors' reluctance to associate with the ancient kings of Rome. The Julio-Claudian dynasty lasted for four emperors, including Tiberius, Caligula, Claudius, and Nero, before the Year of the Four Emperors. Vespasian's Flavian dynasty succeeded the Julio-Claudian dynasty, while the Nerva–Antonine dynasty produced the \"Five Good Emperors\": Nerva, Trajan, Hadrian, Antoninus Pius, and Marcus Aurelius. The accession of the emperor Commodus marked the\n"
     ]
    }
   ],
   "source": [
    "print(summary_ia3)\n",
    "print(\"\\n->\",summary_both)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
